# DÃ©ploiement du modÃ¨le de prÃ©diction du turnover â€” Phase 1 : Pipeline et PostgreSQL

## ğŸ“š Sommaire
- [ğŸ¯ Objectif](#-objectif)
- [ğŸ§  Ã‰tapes clÃ©s](#-Ã©tapes-clÃ©s)
- [ğŸ—„ï¸ Base PostgreSQL](#ï¸-base-postgresql)
- [ğŸ§ª Tests & CI/CD](#-tests--cicd)
- [ğŸ“ Arborescence du dÃ©pÃ´t](#-arborescence-du-dÃ©pÃ´t)
- [ğŸš€ Phase 2 : API et DÃ©ploiement](#-phase-2--api-et-dÃ©ploiement)
- [ğŸ“‚ Documentation complÃ©mentaire](#-documentation-complÃ©mentaire)

---

## ğŸ¯ Objectif

Cette premiÃ¨re phase du projet vise Ã  :
- dÃ©velopper un modÃ¨le de prÃ©diction du dÃ©part volontaire des employÃ©s (turnover),
- structurer le pipeline de traitement (prÃ©paration + modÃ¨le XGBoost),
- **traduire toutes les Ã©tapes de traitement en SQL** pour une exÃ©cution en base PostgreSQL.

---

## ğŸ§  Ã‰tapes clÃ©s

- Analyse exploratoire et feature engineering dans des notebooks Python
- EntraÃ®nement dâ€™un modÃ¨le XGBoost encapsulÃ© dans un pipeline Scikit-learn (`pipeline.joblib`)
- Sauvegarde des colonnes dâ€™entrÃ©e du pipeline (`columns.json`)
- **Reproduction en SQL** des Ã©tapes de nettoyage, fusion, transformation
- Construction de la base PostgreSQL avec :
  - `dataset_final` : table dâ€™entrÃ©e du modÃ¨le
  - `historique_predictions` : journalisation des prÃ©dictions

---

## ğŸ—„ï¸ Base PostgreSQL

La base locale comprend deux tables principales :
- `dataset_final` : donnÃ©es nettoyÃ©es et transformÃ©es Ã  partir de plusieurs fichiers CSV
- `historique_predictions` : enregistrement horodatÃ© de chaque prÃ©diction produite en phase 2

Lâ€™ensemble des scripts SQL (crÃ©ation, jointures, transformation, export) est disponible dans le dossier `sql/`.

---

## ğŸ§ª Tests & CI/CD

Un dÃ©pÃ´t GitHub distant est utilisÃ© pour :
- GÃ©rer le versionnement du code (`main`, `dev`)
- DÃ©clencher un pipeline CI sur chaque `push` ou `pull_request` avec GitHub Actions
- ExÃ©cuter des tests unitaires (prÃ©diction sur donnÃ©es simulÃ©es)

### ğŸ”§ Workflow CI/CD (`.github/workflows/ci.yml`)

- `push` ou `pull_request` sur `main` ou `dev` :
  - `checkout` du code source
  - Installation de Python 3.11
  - Installation des dÃ©pendances avec Poetry (avec cache)
  - ExÃ©cution des tests unitaires Pytest
  - Construction du dossier `build` :
    - `app/`
    - `README.md`
    - `pyproject.toml`
  - Upload du build comme artifact GitHub

---

## ğŸ“ Arborescence du dÃ©pÃ´t

| Dossier/Fichier                     | RÃ´le                                                                 |
|------------------------------------|----------------------------------------------------------------------|
| `notebooks/`                       | Analyse exploratoire et pipeline (`P4_EDA_FE_final`, `P4_Pipeline_ML_final`) |
| `app/models/`                      | Chargement du pipeline, fichiers `pipeline.joblib`, `load_pipeline.py` |
| `sql/`                             | Scripts SQL pour la base de donnÃ©es                                 |
| `docs/`                            | Documentation mÃ©tier et technique                                   |
| `tests/`                           | Test unitaire du pipeline                                           |
| `.github/workflows/ci.yml`        | Configuration de l'intÃ©gration continue (CI)                        |
| `pyproject.toml` / `poetry.lock`  | DÃ©pendances gÃ©rÃ©es avec Poetry                                      |

---

## ğŸš€ Phase 2 : API et DÃ©ploiement

Lâ€™API FastAPI, les tests d'intÃ©gration et le dÃ©ploiement sont traitÃ©s dans un **second dÃ©pÃ´t** :

ğŸ“ `projet-5-deploiement-classification-turn-over-fastapi`

Ce dÃ©pÃ´t **actuel** se concentre sur :
- la production dâ€™un pipeline **reproductible** et **traÃ§able**
- la mise en place dâ€™une base **PostgreSQL** exploitable par lâ€™API

---

## ğŸ“‚ Documentation complÃ©mentaire

- ğŸ§ª [Standards de code et pratiques ML](docs/README-standards.md)
- âš™ï¸ [Workflow CI/CD](docs/ci-cd/definition-workflow.md)
- ğŸ§± [SchÃ©ma relationnel PostgreSQL](docs/sql/schema_base_donnees.md)
