{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c476a20-a706-4dc6-b1a2-312256ee6cef",
   "metadata": {},
   "source": [
    "# Workflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840655c-b6a4-4b5e-8f3b-ca1a53cc04fa",
   "metadata": {},
   "source": [
    "## Import des modules n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77b5cdf-96c5-4d69-9bf8-c3ec1938b4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fab\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/Fab/Documents/P5_D√©ployez_un_mod√®le_de_Machine_Learning\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, fbeta_score, confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import joblib\n",
    "\n",
    "# M√©triques pour la classification\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    fbeta_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# SMOTE\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from app.utils.binary_mapper import BinaryMapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca385e2f-2af8-41d9-9402-e7e0408e04e8",
   "metadata": {},
   "source": [
    "## Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c3d71c-4793-4ccd-a9a3-af09a268dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employes_pipe = pd.read_csv('data/employes_net.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717dbd49-8aed-48d7-bb15-a28f1adb07a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 43 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   age                                        1470 non-null   int64  \n",
      " 1   genre                                      1470 non-null   object \n",
      " 2   revenu_mensuel                             1470 non-null   int64  \n",
      " 3   statut_marital                             1470 non-null   object \n",
      " 4   departement                                1470 non-null   object \n",
      " 5   poste                                      1470 non-null   object \n",
      " 6   nombre_experiences_precedentes             1470 non-null   int64  \n",
      " 7   annee_experience_totale                    1470 non-null   int64  \n",
      " 8   annees_dans_l_entreprise                   1470 non-null   int64  \n",
      " 9   annees_dans_le_poste_actuel                1470 non-null   int64  \n",
      " 10  a_quitte_l_entreprise                      1470 non-null   int64  \n",
      " 11  nombre_participation_pee                   1470 non-null   int64  \n",
      " 12  nb_formations_suivies                      1470 non-null   int64  \n",
      " 13  distance_domicile_travail                  1470 non-null   int64  \n",
      " 14  niveau_education                           1470 non-null   int64  \n",
      " 15  domaine_etude                              1470 non-null   object \n",
      " 16  frequence_deplacement                      1470 non-null   object \n",
      " 17  annees_depuis_la_derniere_promotion        1470 non-null   int64  \n",
      " 18  annes_sous_responsable_actuel              1470 non-null   int64  \n",
      " 19  satisfaction_employee_environnement        1470 non-null   int64  \n",
      " 20  note_evaluation_precedente                 1470 non-null   int64  \n",
      " 21  niveau_hierarchique_poste                  1470 non-null   int64  \n",
      " 22  satisfaction_employee_nature_travail       1470 non-null   int64  \n",
      " 23  satisfaction_employee_equipe               1470 non-null   int64  \n",
      " 24  satisfaction_employee_equilibre_pro_perso  1470 non-null   int64  \n",
      " 25  note_evaluation_actuelle                   1470 non-null   int64  \n",
      " 26  heures_supplementaires                     1470 non-null   object \n",
      " 27  augmentation_salaire_precedente            1470 non-null   float64\n",
      " 28  distance_domicile_travail_qcut             1470 non-null   object \n",
      " 29  tranche_age                                1470 non-null   object \n",
      " 30  ratio_stagnation                           1470 non-null   float64\n",
      " 31  ratio_sous_responsable                     1470 non-null   float64\n",
      " 32  age_revenu                                 1470 non-null   int64  \n",
      " 33  satisfaction_moyenne                       1470 non-null   float64\n",
      " 34  interaction_satisfaction_anciennete        1470 non-null   float64\n",
      " 35  taux_de_formation                          1470 non-null   float64\n",
      " 36  heures_supplementaires_binaire             1470 non-null   int64  \n",
      " 37  interaction_distance_heures_sup            1470 non-null   int64  \n",
      " 38  delta_evaluation                           1470 non-null   int64  \n",
      " 39  revenu_stable                              1470 non-null   int64  \n",
      " 40  delta_eval_x_revenu_stable                 1470 non-null   int64  \n",
      " 41  frequence_deplacement_num                  1470 non-null   int64  \n",
      " 42  surmenage_transports                       1470 non-null   int64  \n",
      "dtypes: float64(6), int64(28), object(9)\n",
      "memory usage: 494.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_employes_pipe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0627bfa8-aea3-42b1-9540-ce12a904a6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_employes_pipe[\"revenu_quartile\"] = df_employes_pipe[\"revenu_quartile\"].astype(\"category\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_employes_pipe[\"distance_domicile_travail_qcut\"] = df_employes_pipe[\"distance_domicile_travail_qcut\"].astype(\"category\")\n",
    "'''df_employes_pipe[\"revenu_quartile\"] = df_employes_pipe[\"revenu_quartile\"].astype(\"category\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003d04d-ee47-4166-8461-d78b98bdd830",
   "metadata": {},
   "source": [
    "## S√©lection des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f83db7-f2fd-43ca-8ff4-400d5e94935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables num√©riques continues ‚Äì MinMaxScaler\n",
    "# Nouvelle liste num_MinMax_vars incluant toutes les colonnes num√©riques continues\n",
    "num_MinMax_vars = [\n",
    "    \"revenu_mensuel\",\n",
    "    \"age\",\n",
    "    \"annee_experience_totale\",\n",
    "    \"annees_dans_l_entreprise\",\n",
    "    \"annees_dans_le_poste_actuel\",\n",
    "    \"annes_sous_responsable_actuel\",\n",
    "    \"nombre_participation_pee\",\n",
    "    \"ratio_stagnation\",\n",
    "    \"ratio_sous_responsable\",\n",
    "    \"age_revenu\",\n",
    "    \"satisfaction_moyenne\",\n",
    "    \"interaction_satisfaction_anciennete\",\n",
    "    \"taux_de_formation\",\n",
    "    \"interaction_distance_heures_sup\",\n",
    "    \"surmenage_transports\",\n",
    "    # Les 10 colonnes suppl√©mentaires\n",
    "    \"revenu_stable\",\n",
    "    \"distance_domicile_travail\",\n",
    "    \"nb_formations_suivies\",\n",
    "    \"delta_eval_x_revenu_stable\",\n",
    "    \"delta_evaluation\",\n",
    "    \"nombre_experiences_precedentes\",\n",
    "    \"annees_depuis_la_derniere_promotion\",\n",
    "    \"frequence_deplacement_num\",\n",
    "    \"augmentation_salaire_precedente\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9eb929f-7e7f-46ff-a43d-efbc6c991925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables cat√©gorielles nominales - OneHotEncoder\n",
    "cat_nominal_onehot_vars = [\n",
    "    \"statut_marital\",\n",
    "    \"departement\",\n",
    "    \"poste\",\n",
    "    \"domaine_etude\",\n",
    "    \"distance_domicile_travail_qcut\",\n",
    "    \"tranche_age\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c2d2b3-c19c-4fca-b66f-eee2016e1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables cat√©gorielles ordinales num√©riques ‚Äì OrdinalEncoder\n",
    "cat_ordinal_numeric_vars = [\n",
    "    \"niveau_education\",\n",
    "    \"satisfaction_employee_environnement\",\n",
    "    \"note_evaluation_precedente\",\n",
    "    \"niveau_hierarchique_poste\",\n",
    "    \"satisfaction_employee_nature_travail\",\n",
    "    \"satisfaction_employee_equipe\",\n",
    "    \"satisfaction_employee_equilibre_pro_perso\",\n",
    "    \"note_evaluation_actuelle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58605eb3-e313-4ca4-a4d5-09b7a29794f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat_ordinal_texte_vars = [\"frequence_deplacement\", \"revenu_quartile\"]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables cat√©gorielles ordinales texte ‚Äì OrdinalEncoder\n",
    "cat_ordinal_texte_vars = [\"frequence_deplacement\"]\n",
    "'''cat_ordinal_texte_vars = [\"frequence_deplacement\", \"revenu_quartile\"]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4ab0f5-8bf1-47ff-a13a-baf4726fe803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a ransformer en binaire (0/1)\n",
    "binary_mapping = {\n",
    "    \"genre\": {\"M\": 1, \"F\": 0},\n",
    "    \"heures_supplementaires\": {\"Oui\": 1, \"Non\": 0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09acfa44-2f7a-41ec-a374-94f546c6f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables d√©j√† binaires \n",
    "binary_vars = [\n",
    "    \"heures_supplementaires_binaire\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc34af5b-ac35-4158-a837-6d8145f810d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable cible\n",
    "target_var = \"a_quitte_l_entreprise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7c6f3e-df35-44a2-b9da-ae9dec7f14d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 43 columns):\n",
      " #   Column                                     Non-Null Count  Dtype   \n",
      "---  ------                                     --------------  -----   \n",
      " 0   age                                        1470 non-null   int64   \n",
      " 1   genre                                      1470 non-null   object  \n",
      " 2   revenu_mensuel                             1470 non-null   int64   \n",
      " 3   statut_marital                             1470 non-null   object  \n",
      " 4   departement                                1470 non-null   object  \n",
      " 5   poste                                      1470 non-null   object  \n",
      " 6   nombre_experiences_precedentes             1470 non-null   int64   \n",
      " 7   annee_experience_totale                    1470 non-null   int64   \n",
      " 8   annees_dans_l_entreprise                   1470 non-null   int64   \n",
      " 9   annees_dans_le_poste_actuel                1470 non-null   int64   \n",
      " 10  a_quitte_l_entreprise                      1470 non-null   int64   \n",
      " 11  nombre_participation_pee                   1470 non-null   int64   \n",
      " 12  nb_formations_suivies                      1470 non-null   int64   \n",
      " 13  distance_domicile_travail                  1470 non-null   int64   \n",
      " 14  niveau_education                           1470 non-null   int64   \n",
      " 15  domaine_etude                              1470 non-null   object  \n",
      " 16  frequence_deplacement                      1470 non-null   object  \n",
      " 17  annees_depuis_la_derniere_promotion        1470 non-null   int64   \n",
      " 18  annes_sous_responsable_actuel              1470 non-null   int64   \n",
      " 19  satisfaction_employee_environnement        1470 non-null   int64   \n",
      " 20  note_evaluation_precedente                 1470 non-null   int64   \n",
      " 21  niveau_hierarchique_poste                  1470 non-null   int64   \n",
      " 22  satisfaction_employee_nature_travail       1470 non-null   int64   \n",
      " 23  satisfaction_employee_equipe               1470 non-null   int64   \n",
      " 24  satisfaction_employee_equilibre_pro_perso  1470 non-null   int64   \n",
      " 25  note_evaluation_actuelle                   1470 non-null   int64   \n",
      " 26  heures_supplementaires                     1470 non-null   object  \n",
      " 27  augmentation_salaire_precedente            1470 non-null   float64 \n",
      " 28  distance_domicile_travail_qcut             1470 non-null   category\n",
      " 29  tranche_age                                1470 non-null   object  \n",
      " 30  ratio_stagnation                           1470 non-null   float64 \n",
      " 31  ratio_sous_responsable                     1470 non-null   float64 \n",
      " 32  age_revenu                                 1470 non-null   int64   \n",
      " 33  satisfaction_moyenne                       1470 non-null   float64 \n",
      " 34  interaction_satisfaction_anciennete        1470 non-null   float64 \n",
      " 35  taux_de_formation                          1470 non-null   float64 \n",
      " 36  heures_supplementaires_binaire             1470 non-null   int64   \n",
      " 37  interaction_distance_heures_sup            1470 non-null   int64   \n",
      " 38  delta_evaluation                           1470 non-null   int64   \n",
      " 39  revenu_stable                              1470 non-null   int64   \n",
      " 40  delta_eval_x_revenu_stable                 1470 non-null   int64   \n",
      " 41  frequence_deplacement_num                  1470 non-null   int64   \n",
      " 42  surmenage_transports                       1470 non-null   int64   \n",
      "dtypes: category(1), float64(6), int64(28), object(8)\n",
      "memory usage: 484.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_employes_pipe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cea1fa-86b9-4cd0-b594-0e76e2786009",
   "metadata": {},
   "source": [
    "## Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09cf91d7-432d-4943-a761-ba1205080c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_employes_pipe.drop(columns=[target_var])\n",
    "y = df_employes_pipe[target_var]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02559f05-4d68-4610-8e6f-bf6fbaaea617",
   "metadata": {},
   "source": [
    "## D√©finition des sous-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fff9fd7b-7518-45c0-8fee-8c6c5bc01fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Cr√©ation d'une classe de transformation pour le genre\\nclass BinaryMapper(BaseEstimator, TransformerMixin):\\n    def __init__(self, mapping=None):\\n        # mapping doit √™tre un dict : {colonne: {valeur: code, ...}, ...}\\n        self.mapping = mapping or {} \\n\\n    def fit(self, X, y=None):\\n        return self  # pas d'apprentissage n√©cessaire\\n\\n    def transform(self, X):\\n        X_ = X.copy()\\n        for col, col_map in self.mapping.items():\\n            X_[col] = X_[col].map(col_map)\\n        return X_\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Cr√©ation d'une classe de transformation pour le genre\n",
    "class BinaryMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mapping=None):\n",
    "        # mapping doit √™tre un dict : {colonne: {valeur: code, ...}, ...}\n",
    "        self.mapping = mapping or {} \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # pas d'apprentissage n√©cessaire\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for col, col_map in self.mapping.items():\n",
    "            X_[col] = X_[col].map(col_map)\n",
    "        return X_'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c18d57-00cc-4659-a015-a5a4268f9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On d√©clare les valeurs binaires pour le genre :\n",
    "binary_mapping = {\n",
    "    \"genre\": {\"M\": 1, \"F\": 0},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6857da74-10f4-41a7-bfe3-0416ee5e0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sous-pipeline de normalisation\n",
    "normalisation_pipeline = Pipeline([\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Sous-pipeline de OneHotEncoder\n",
    "hot_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Sous-pipeline OrdinalEncoder sur variables num√©riques ordonn√©es\n",
    "ordinal_num_pipeline = Pipeline([\n",
    "    ('ordinal_num', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# Sous-pipeline OrdinalEncoder sur variables textuelles explicitement ordonn√©es\n",
    "ordinal_texte_pipeline = Pipeline([\n",
    "    ('ordinal_texte', OrdinalEncoder(categories=[\n",
    "        ['Aucun', 'Occasionnel', 'Frequent'],\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Sous-pipeline de binarisation du genre uniquement\n",
    "binary_pipeline = Pipeline([\n",
    "    ('binariseur', BinaryMapper(mapping=binary_mapping))\n",
    "])\n",
    "\n",
    "# Sous-pipeline \n",
    "binary_pass_pipeline = Pipeline([\n",
    "    ('pass_through', FunctionTransformer(validate=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc56b89-92e0-4987-bd5f-075f95779847",
   "metadata": {},
   "source": [
    "## Assemblage avec ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e8726b-e3dd-42ec-af8c-73cbd3244b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemblage\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('normal', normalisation_pipeline, num_MinMax_vars),\n",
    "    ('hot', hot_pipeline, cat_nominal_onehot_vars),\n",
    "    ('ordinal_num', ordinal_num_pipeline, cat_ordinal_numeric_vars),\n",
    "    ('ordinal_texte', ordinal_texte_pipeline, cat_ordinal_texte_vars),\n",
    "    ('binary_mapper', binary_pipeline, [\"genre\", \"heures_supplementaires\"]),\n",
    "    ('binary_pass', binary_pass_pipeline, binary_vars)\n",
    "],\n",
    "                                 remainder=\"drop\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485f5bf-fd93-4118-b76b-88a7fbf578d0",
   "metadata": {},
   "source": [
    "## Pipeline complet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a4230-cbdd-4f3c-a023-997b729172f7",
   "metadata": {},
   "source": [
    "### S√©lection des m√©triques et fonction d'affichage des valeurs de m√©triques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92094bd-48f6-4b83-8cc0-8f3593366f01",
   "metadata": {},
   "source": [
    "### Choix des m√©triques de classification\n",
    "\n",
    "- **Accuracy** : Pourcentage de pr√©dictions correctes globalement.\n",
    "- **Pr√©cision** : Parmi les employ√©s pr√©dits comme partants, combien le sont r√©ellement. Important si faux positifs co√ªteux.\n",
    "- **Rappel** : Parmi les employ√©s partis, combien sont bien d√©tect√©s. Important si faux n√©gatifs co√ªteux.\n",
    "- **F-beta score** : Combine pr√©cision et rappel avec un poids ajustable selon la priorit√© (par d√©faut beta=1 = F1).\n",
    "- **Matrice de confusion** : Pour voir le d√©tail des erreurs (Faux Positifs, Faux N√©gatifs).\n",
    "- **Classification report** : Pour une synth√®se compl√®te des m√©triques par classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcfe7db3-c82b-4195-b504-9c424233ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratifiedKFold avec 5 splits\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb0bcb5d-39b3-4fad-827f-f8465f2438c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les scores par mod√®le\n",
    "resultats_cv = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fae84-dec8-462d-a7d9-6892c6aff4a9",
   "metadata": {},
   "source": [
    "***‚öôÔ∏è D√©marche de comparaison \"out of the box\" et validation crois√©e***\n",
    "\n",
    "Avant d'engager une optimisation approfondie des hyperparam√®tres (Grid Search), nous allons comparer plusieurs mod√®les de classification \"out of the box\".  \n",
    "Cela signifie que nous les utilisons **avec leurs param√®tres par d√©faut**, sans r√©glage sp√©cifique.  \n",
    "\n",
    "L'objectif est de :\n",
    "- Obtenir une **premi√®re mesure neutre de performance**, pour chaque algorithme.\n",
    "- Identifier rapidement les mod√®les les plus prometteurs.\n",
    "- V√©rifier la stabilit√© des performances via une validation crois√©e.\n",
    "\n",
    "Nous avons ainsi mis en place une **validation crois√©e √† 5 folds** sur les donn√©es d'entra√Ænement.  \n",
    "Pour chaque mod√®le :\n",
    "- Les donn√©es sont d√©coup√©es en 5 sous-ensembles (folds).\n",
    "- √Ä chaque it√©ration, le mod√®le est entra√Æn√© sur 4 folds et √©valu√© sur le fold restant.\n",
    "- Nous calculons 4 m√©triques principales : Accuracy, Pr√©cision, Rappel et F2-score.\n",
    "- Nous stockons la moyenne et l'√©cart-type de chaque m√©trique.\n",
    "\n",
    "Cette approche progressive nous permet :\n",
    "- De comparer objectivement les mod√®les sur une base identique.\n",
    "- De rep√©rer ceux qui pr√©sentent un potentiel avant le fine-tuning.\n",
    "- D'√©viter de passer du temps √† optimiser un mod√®le qui serait d√©j√† en difficult√© dans sa version standard.\n",
    "\n",
    "En r√©sum√©, la validation crois√©e \"out of the box\" constitue un **premier filtre exploratoire** avant de lancer la recherche approfondie des meilleurs hyperparam√®tres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5940014a-fbd2-4bee-90b9-03172d3f1bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Validation crois√©e pour le mod√®le : Mod√®le Dummy ====\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "==== Validation crois√©e pour le mod√®le : Random Forest Classifier ====\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Non'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m--------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     36\u001b[39m pipeline_cv = Pipeline([\n\u001b[32m     37\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m     38\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mmodele_utilise\u001b[39m\u001b[33m'\u001b[39m, modele)\n\u001b[32m     39\u001b[39m ])\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Entra√Æner\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mpipeline_cv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Pr√©dire (seuil par d√©faut 0.5)\u001b[39;00m\n\u001b[32m     45\u001b[39m y_pred = pipeline_cv.predict(X_fold_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\imblearn\\pipeline.py:526\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    521\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    522\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    523\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    524\u001b[39m             all_params=params,\n\u001b[32m    525\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:360\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[32m    373\u001b[39m estimator = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator)(criterion=\u001b[38;5;28mself\u001b[39m.criterion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1053\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1058\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-d√©ployez-un-mod√®le-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    837\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Non'"
     ]
    }
   ],
   "source": [
    "modeles = {\n",
    "    \"Mod√®le Dummy\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(random_state=42),\n",
    "    \"Regression Logistique\": LogisticRegression(random_state=42),\n",
    "    \"XGBoost Classifier\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"CatBoost Classifier\": CatBoostClassifier(\n",
    "        verbose=0,   # Pas d'affichage des logs pendant le fit. CatBoost affiche par d√©faut beaucoup de logs.Ici on les d√©sactive pour que la boucle reste lisible.\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "for nom_modele, modele in modeles.items():\n",
    "    print(f\"\\n==== Validation crois√©e pour le mod√®le : {nom_modele} ====\")\n",
    "    \n",
    "    # Initialiser les listes pour stocker les scores\n",
    "    scores_accuracy = []\n",
    "    scores_precision = []\n",
    "    scores_recall = []\n",
    "    scores_f2 = []\n",
    "    \n",
    "    # Boucle sur les folds\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(f\"\\nFold {fold+1}/5\")\n",
    "        \n",
    "        # S√©parer les donn√©es\n",
    "        X_fold_train = X_train.iloc[train_idx]\n",
    "        y_fold_train = y_train.iloc[train_idx]\n",
    "        X_fold_val = X_train.iloc[val_idx]\n",
    "        y_fold_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        # Cr√©er un pipeline propre √† chaque fold        \n",
    "        pipeline_cv = Pipeline([\n",
    "            ('preprocessing', preprocessor),\n",
    "            ('modele_utilise', modele)\n",
    "        ])\n",
    "                             \n",
    "        # Entra√Æner\n",
    "        pipeline_cv.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Pr√©dire (seuil par d√©faut 0.5)\n",
    "        y_pred = pipeline_cv.predict(X_fold_val)\n",
    "        \n",
    "        # Calculer les m√©triques\n",
    "        acc = accuracy_score(y_fold_val, y_pred)\n",
    "        prec = precision_score(y_fold_val, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_fold_val, y_pred, zero_division=0)\n",
    "        f2 = fbeta_score(y_fold_val, y_pred, beta=2, zero_division=0)\n",
    "        \n",
    "        # Stocker\n",
    "        scores_accuracy.append(acc)\n",
    "        scores_precision.append(prec)\n",
    "        scores_recall.append(rec)\n",
    "        scores_f2.append(f2)\n",
    "    \n",
    "    # R√©sumer\n",
    "    resultats_cv[nom_modele] = {\n",
    "        'Accuracy': (np.mean(scores_accuracy), np.std(scores_accuracy)),\n",
    "        'Precision': (np.mean(scores_precision), np.std(scores_precision)),\n",
    "        'Recall': (np.mean(scores_recall), np.std(scores_recall)),\n",
    "        'F2-score': (np.mean(scores_f2), np.std(scores_f2))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b052f3-0c30-40b5-bf07-d0dfc9bca29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour le dataframe\n",
    "tableau_resultats = []\n",
    "\n",
    "for modele, metriques in resultats_cv.items():\n",
    "    ligne = {\n",
    "        'Mod√®le': modele,\n",
    "        'Accuracy (moy ¬± std)': f\"{metriques['Accuracy'][0]:.3f} ¬± {metriques['Accuracy'][1]:.3f}\",\n",
    "        'Precision (moy ¬± std)': f\"{metriques['Precision'][0]:.3f} ¬± {metriques['Precision'][1]:.3f}\",\n",
    "        'Recall (moy ¬± std)': f\"{metriques['Recall'][0]:.3f} ¬± {metriques['Recall'][1]:.3f}\",\n",
    "        'F2-score (moy ¬± std)': f\"{metriques['F2-score'][0]:.3f} ¬± {metriques['F2-score'][1]:.3f}\"\n",
    "    }\n",
    "    tableau_resultats.append(ligne)\n",
    "\n",
    "df_resultats_cv = pd.DataFrame(tableau_resultats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d19b0-fbe9-40da-8c75-f56a6937ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultats_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415fa6e-cb25-4fa9-861c-cc90e20c7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des mod√®les et des m√©triques\n",
    "modeles = df_resultats_cv['Mod√®le'].tolist()\n",
    "metriques = ['Accuracy', 'Precision', 'Recall', 'F2-score']\n",
    "\n",
    "# Extraction des moyennes et √©carts-types\n",
    "moyennes = {m: [] for m in metriques}\n",
    "ecarts = {m: [] for m in metriques}\n",
    "\n",
    "for m in metriques:\n",
    "    for i in range(len(df_resultats_cv)):\n",
    "        # La colonne s'appelle ex: \"Accuracy (moy ¬± std)\"\n",
    "        texte = df_resultats_cv.iloc[i][f\"{m} (moy ¬± std)\"]\n",
    "        moy, std = texte.split('¬±')\n",
    "        moyennes[m].append(float(moy.strip()))\n",
    "        ecarts[m].append(float(std.strip()))\n",
    "\n",
    "# Param√®tres du graphique\n",
    "x = np.arange(len(modeles))  # positions des mod√®les\n",
    "largeur = 0.2  # largeur de chaque barre\n",
    "\n",
    "# Cr√©ation de la figure\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Pour chaque m√©trique, cr√©er une s√©rie de barres\n",
    "for idx, m in enumerate(metriques):\n",
    "    positions = x + (idx - 1.5)*largeur\n",
    "    plt.bar(\n",
    "        positions,\n",
    "        moyennes[m],\n",
    "        width=largeur,\n",
    "        yerr=ecarts[m],\n",
    "        capsize=4,\n",
    "        label=m\n",
    "    )\n",
    "\n",
    "plt.xticks(x, modeles, rotation=15)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"Score moyen (cross-validation)\")\n",
    "plt.title(\"Comparaison des performances des mod√®les (moyennes ¬± √©carts-types)\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464469c9-4dcf-404c-91c4-a9fe842adefb",
   "metadata": {},
   "source": [
    "Nous allons retenir **XGBoostClassifier** car il pr√©sente les meilleures performances parmi les mod√®les non-lin√©aires (exigence du projet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997cde8-fe4a-4216-9049-2770bf3c833f",
   "metadata": {},
   "source": [
    "## Optimisation des hyperparam√®tres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e67c19-3a65-4ced-8b97-0c2dce63e02d",
   "metadata": {},
   "source": [
    "### D√©finition de la grille d'hyperparam√®tres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c394c-da1a-453a-97f8-2c77cf536904",
   "metadata": {},
   "source": [
    "### üéØ Fine-tuning de XGBoostClassifier : choix des hyperparam√®tres\n",
    "\n",
    "Pour optimiser notre XGBoostClassifier, nous avons cibl√© **quatre hyperparam√®tres prioritaires**, reconnus comme les plus influents dans la litt√©rature et la pratique‚ÄØ:\n",
    "\n",
    "- **max_depth** (profondeur maximale des arbres)  \n",
    "  Permet de moduler la complexit√© du mod√®le.  \n",
    "  Des valeurs plus √©lev√©es captent plus de d√©tails, mais augmentent le risque de surapprentissage.\n",
    "  \n",
    "- **learning_rate** (alias *eta*, taux d‚Äôapprentissage)  \n",
    "  Contr√¥le l‚Äôamplitude des mises √† jour du mod√®le.  \n",
    "  Un learning rate faible ralentit l‚Äôapprentissage mais favorise une convergence plus stable.\n",
    "  \n",
    "- **subsample**  \n",
    "  Fraction des √©chantillons utilis√©s par arbre.  \n",
    "  Introduit une forme de *bagging* qui r√©duit la variance et limite le surapprentissage.  \n",
    "  Le *bagging* consiste √† entra√Æner plusieurs mod√®les sur des sous-√©chantillons diff√©rents, puis √† agr√©ger leurs pr√©dictions, ce qui stabilise les r√©sultats.\n",
    "  \n",
    "- **scale_pos_weight**  \n",
    "  Coefficient de pond√©ration des observations de la classe minoritaire.  \n",
    "  Ce param√®tre est particuli√®rement utile dans un contexte de classes d√©s√©quilibr√©es.  \n",
    "  Il permet d‚Äôindiquer √† l‚Äôalgorithme qu‚Äôune erreur sur la classe positive est plus p√©nalisante.  \n",
    "  Cela peut am√©liorer le rappel sans recourir syst√©matiquement √† l‚Äôoversampling.\n",
    "  \n",
    "Trois de ces quatre param√®tres sont mis en avant comme le **\"trio cl√©\"** √† optimiser en priorit√©, notamment dans cet [Article source](https://medium.com/%40nekhumbecolbert3/unleashing-the-power-of-catboostclassifier-a-robust-model-for-categorical-feature-handling-a05e7dcf23b2) (Unleashing the Power of CatBoostClassifier: A Robust Model for Categorical Feature Handling de Humbulani Colbert, Medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e60ff6-a7fe-4273-a77b-63539aa77985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des hyperparam√®tres\n",
    "param_grid_xgboost = {\n",
    "    'modele_utilise__max_depth': [3, 6, 9],\n",
    "    'modele_utilise__learning_rate': [0.1, 0.3, 0.5],\n",
    "    'modele_utilise__subsample': [0.25, 0.5, 1.0],\n",
    "    'modele_utilise__scale_pos_weight': [1, 3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dffde6-1e11-4df8-ab34-87be354ee0ba",
   "metadata": {},
   "source": [
    "##### üîç Raisonnement derri√®re les choix de valeurs\n",
    "\n",
    "Nous avons appliqu√© une m√©thode raisonn√©e consistant √† explorer, pour chaque hyperparam√®tre, **la valeur par d√©faut et deux variantes contrast√©es** (une plus prudente, une plus ambitieuse).  \n",
    "Cette approche permet d‚Äôidentifier l‚Äôinfluence des r√©glages sans partir dans un grid trop vaste.\n",
    "\n",
    "---\n",
    "\n",
    "##### üìä Hyperparam√®tres de complexit√© et r√©gularisation\n",
    "\n",
    "| Hyperparam√®tre    | Valeur par d√©faut      | Variante ‚Äú-1‚Äù              | Variante ‚Äú+1‚Äù               |\n",
    "|-------------------|------------------------|----------------------------|-----------------------------|\n",
    "| **max_depth**     | 6                      | 3 (mod√®le moins complexe)  | 9 (mod√®le plus complexe)    |\n",
    "| **learning_rate** | 0.3                    | 0.1 (apprentissage plus r√©gulier) | 0.5 (apprentissage plus rapide) |\n",
    "| **subsample**     | 1.0                    | 0.5 (bagging plus fort)    | 1.0 (bagging complet)       |\n",
    "\n",
    "Ces valeurs sont directement inspir√©es des recommandations courantes pour **ma√Ætriser le compromis biais/variance** et tester l‚Äôimpact de la profondeur et de la vitesse d‚Äôapprentissage.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚öñÔ∏è Hyperparam√®tre de pond√©ration des classes\n",
    "\n",
    "| Hyperparam√®tre         | Valeur par d√©faut | Variante ‚Äú+1‚Äù          | Variante ‚Äú+2‚Äù           |\n",
    "|------------------------|-------------------|------------------------|-------------------------|\n",
    "| **scale_pos_weight**   | 1                 | 3 (pond√©ration mod√©r√©e) | 5 (pond√©ration plus forte) |\n",
    "\n",
    "**Pourquoi cet encadrement diff√©rent ?**\n",
    "\n",
    "- Contrairement aux autres hyperparam√®tres, `scale_pos_weight` **ne r√©gule pas la complexit√©**, mais **r√©√©quilibre la p√©nalisation des classes dans la fonction de perte**.\n",
    "- Les bonnes pratiques recommandent de tester plusieurs valeurs sup√©rieures √† 1 lorsque la classe positive est minoritaire.\n",
    "- M√™me si notre taux de d√©part est ‚âà16‚ÄØ% (et non ultra-d√©s√©quilibr√©), il √©tait pertinent d‚Äôexaminer si une pond√©ration pouvait am√©liorer le rappel.\n",
    "- Nous avons volontairement choisi des valeurs **uniquement sup√©rieures √† la r√©f√©rence (1)**, car la pond√©ration inf√©rieure n‚Äôa pas de sens pratique ici.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "Cette structuration permet de :\n",
    "- Conserver un grid limit√© et rationnel.\n",
    "- S√©parer clairement les param√®tres qui contr√¥lent la complexit√© et ceux qui agissent sur le d√©s√©quilibre.\n",
    "- Justifier en entretien que chaque hyperparam√®tre a √©t√© r√©fl√©chi en fonction de son r√¥le sp√©cifique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1eac7d-7710-4a90-9699-8e30039745a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'un scorer personnalis√© qui calcule le F2-score (pond√®re davantage le rappel) √† chaque fold\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Pipeline complet\n",
    "# ‚ÄºÔ∏è1er essai scale_pos_weight sans SMOTE\n",
    "pipeline_XGB_no_smote = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('modele_utilise', XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_XGB_no_smote,\n",
    "    param_grid=param_grid_xgboost,\n",
    "    scoring=f2_scorer,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Lancement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# R√©sultats\n",
    "print(\"‚úÖ Meilleurs hyperparam√®tres trouv√©s :\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\n‚úÖ Meilleur F2-score moyen (CV) :\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75c9ba-747e-4d2b-b345-8cf56dc89c89",
   "metadata": {},
   "source": [
    "##### üîç R√©sultats du Grid Search\n",
    "\n",
    "**Synth√®se des hyperparam√®tres retenus :**\n",
    "\n",
    "- `learning_rate = 0.1`  \n",
    "  Le taux d‚Äôapprentissage le plus faible test√©.  \n",
    "  \n",
    "- `max_depth = 3`  \n",
    "  La profondeur minimale parmi les valeurs test√©es.  \n",
    "  Cela montre qu‚Äôun mod√®le peu complexe suffit √† capturer les patterns principaux, tout en limitant le risque de surapprentissage.\n",
    "  \n",
    "- `subsample = 0.25`  \n",
    "  La fraction la plus r√©duite des √©chantillons par arbre.  \n",
    "  Ce bagging fort introduit davantage de diversit√© entre les arbres.\n",
    "  \n",
    "- `scale_pos_weight = 5`  \n",
    "  La pond√©ration maximale explor√©e pour la classe minoritaire.  \n",
    "  Cela confirme que renforcer l‚Äôattention port√©e aux d√©parts am√©liore le rappel, ce qui est coh√©rent avec l‚Äôobjectif m√©tier.\n",
    "\n",
    "**Performance obtenue :**\n",
    "\n",
    "- Le **F2-score moyen sur validation crois√©e est de 0.534**, ce qui constitue une am√©lioration significative par rapport √† la plupart des configurations ‚Äúout of the box‚Äù.\n",
    "- Ce score refl√®te un bon compromis rappel/pr√©cision, avec un accent mis sur le rappel comme souhait√©.\n",
    "\n",
    "**Interpr√©tation :**\n",
    "\n",
    "- Ces r√©glages illustrent qu‚Äôen contexte de d√©s√©quilibre de classes, il est souvent pr√©f√©rable :\n",
    "  - De privil√©gier des mod√®les simples et r√©guliers (`max_depth` faible, `learning_rate` bas).\n",
    "  - D‚Äôajouter une forte pond√©ration des observations minoritaires (`scale_pos_weight` √©lev√©).\n",
    "  - De renforcer la diversit√© par le sous-√©chantillonnage (`subsample` faible).\n",
    "\n",
    "En r√©sum√©, le Grid Search a permis de stabiliser les performances et d‚Äôidentifier des hyperparam√®tres coh√©rents avec les contraintes du projet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681777d-bd2d-4d51-9c1a-fce3bddeef99",
   "metadata": {},
   "source": [
    "## Optimisation du seuil de d√©cision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994feb4f-17cd-4a4c-9c44-d1c923dfea18",
   "metadata": {},
   "source": [
    "### Courbe ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfc1e3-18eb-41dc-9e28-fe5c68b0bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les probabilit√©s de la classe positive\n",
    "y_scores_test = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculer la courbe ROC\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_scores_test)\n",
    "\n",
    "# Calculer l'AUC de la ROC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"grey\", label=\"Random classifier\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02868483-2a52-4938-859e-41c888eb56ec",
   "metadata": {},
   "source": [
    "##### Analyse de la courbe ROC\n",
    "\n",
    "La courbe ROC pr√©sent√©e ci-dessus mesure la capacit√© du mod√®le √† distinguer les classes **d√©part** et **non-d√©part**.  \n",
    "\n",
    "‚úÖ **Principaux constats :**\n",
    "- La courbe ROC s‚Äô√©loigne bien de la diagonale al√©atoire, indiquant que le mod√®le a une capacit√© discriminante.\n",
    "- La **surface sous la courbe (AUC) est de 0,79**, ce qui refl√®te une performance globale correcte.\n",
    "\n",
    "‚ö†Ô∏è **√âl√©ments de prudence :**\n",
    "- La courbe ROC n‚Äôindique pas si le mod√®le est meilleur pour limiter les **faux positifs** (pr√©dire un d√©part √† tort) ou les **faux n√©gatifs** (ne pas d√©tecter un d√©part).\n",
    "- Comme les donn√©es sont d√©s√©quilibr√©es (beaucoup moins de d√©parts que de non-d√©parts), la ROC peut donner une impression **trop optimiste de la qualit√© pr√©dictive**.\n",
    "  \n",
    "‚û°Ô∏è **Prochaines √©tapes :**\n",
    "C‚Äôest pourquoi nous allons maintenant analyser la **courbe Pr√©cision‚ÄìRappel**, plus adapt√©e √† l‚Äô√©valuation des mod√®les sur des jeux de donn√©es d√©s√©quilibr√©s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e0b38-7c59-4cee-874f-1bc35239f5bc",
   "metadata": {},
   "source": [
    "### Courbes Pr√©cision-Rappel et F2-score vs seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859c7b0-195d-49fc-ac9b-20e24eb9a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les probabilit√©s de la classe positive\n",
    "y_scores_test = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores_test)\n",
    "\n",
    "# Calculer le F2-score pour chaque seuil\n",
    "f2_scores = []\n",
    "for thresh in thresholds:\n",
    "    preds = (y_scores_test >= thresh).astype(int)\n",
    "    score = fbeta_score(y_test, preds, beta=2)\n",
    "    f2_scores.append(score)\n",
    "\n",
    "# Trouver le seuil optimal\n",
    "best_idx = np.argmax(f2_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f2 = f2_scores[best_idx]\n",
    "\n",
    "print(f\"Seuil optimal : {best_threshold:.4f}\")\n",
    "print(f\"Meilleur F2-score : {best_f2:.4f}\")\n",
    "\n",
    "# Tracer la courbe Pr√©cision‚ÄìRappel\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve - Jeu de Test\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Tracer la courbe F2-score en fonction du seuil\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(thresholds, f2_scores, label=\"F2-score by Threshold\")\n",
    "plt.axvline(best_threshold, color='red', linestyle='--', label=f\"Best threshold = {best_threshold:.2f}\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F2-score\")\n",
    "plt.title(\"F2-score vs Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844e6a3-1dcb-4526-ade0-72aaaad78544",
   "metadata": {},
   "source": [
    "##### Analyse de la courbe Pr√©cision‚ÄìRappel\n",
    "\n",
    "La courbe Pr√©cision‚ÄìRappel permet d‚Äô√©valuer la performance du mod√®le sur un jeu de donn√©es **d√©s√©quilibr√©**, en se concentrant sur la capacit√© √† d√©tecter les d√©parts.\n",
    "\n",
    "‚úÖ **Principaux constats :**\n",
    "- La pr√©cision diminue progressivement lorsque le rappel augmente, ce qui est un comportement attendu.\n",
    "- Pour des valeurs de rappel faibles (<0,2), la pr√©cision peut d√©passer **0,8**, traduisant que lorsqu‚Äôon d√©tecte peu de d√©parts, ils sont souvent corrects.\n",
    "- Lorsque le rappel d√©passe **0,5**, la pr√©cision chute autour de **0,4**, ce qui signifie que plus on cherche √† capter de cas positifs, plus on g√©n√®re de faux positifs.\n",
    "\n",
    "‚ö†Ô∏è **Limites et interpr√©tation :**\n",
    "- Cette courbe met en √©vidence un compromis clair entre **pr√©cision** (qualit√© des alertes) et **rappel** (couverture des d√©parts).\n",
    "- Pour un usage op√©rationnel (ex. ciblage des salari√©s √† risque), il sera crucial de choisir un seuil adapt√© en fonction de la priorit√© :\n",
    "  - **Minimiser les faux positifs** (√©viter d‚Äôalerter inutilement)\n",
    "  - ou **maximiser le rappel** (ne rater aucun d√©part).\n",
    "\n",
    "**Synth√®se :**\n",
    "- La performance est correcte mais montre que le mod√®le a des limites d√®s qu‚Äôon souhaite capturer un grand nombre de d√©parts.\n",
    "- Cette analyse compl√®te la ROC et fournit une vision plus r√©aliste dans le contexte des classes d√©s√©quilibr√©es.\n",
    "\n",
    "##### Analyse de la courbe F2-Score vs Seuil\n",
    "\n",
    "Cette courbe montre comment le **F2-score** √©volue en fonction du seuil de d√©cision appliqu√© aux probabilit√©s pr√©dites.\n",
    "\n",
    "‚úÖ **Principaux constats :**\n",
    "- Le **F2-score maximal (~0,65)** est obtenu pour un seuil autour de **0,24** (ligne rouge).\n",
    "  - Ce seuil favorise le rappel par rapport √† la pr√©cision, ce qui est coh√©rent avec l‚Äôutilisation du F2-score (pond√®re davantage le rappel).\n",
    "- Pour des seuils plus √©lev√©s (>0,4), le F2-score baisse progressivement jusqu‚Äô√† atteindre des valeurs proches de z√©ro.\n",
    "  - Cela indique que fixer un seuil trop strict conduit √† manquer beaucoup de d√©parts.\n",
    "\n",
    "‚ö†Ô∏è **Interpr√©tation et pr√©cautions :**\n",
    "- Le seuil optimal ici est bien **inf√©rieur √† 0,5**, ce qui montre que le mod√®le a tendance √† produire des probabilit√©s faibles m√™me pour les cas positifs.\n",
    "- Ce seuil sera choisi en coh√©rence avec les priorit√©s m√©tier :\n",
    "  - **Maximiser le rappel** (ne pas rater de d√©parts) tout en conservant une pr√©cision acceptable.\n",
    "- Un seuil bas entra√Ænera un nombre plus √©lev√© de faux positifs, qu‚Äôil faudra expliquer et justifier lors de la mise en production.\n",
    "\n",
    "**Synth√®se :**\n",
    "- La courbe permet de s√©lectionner un seuil adapt√© aux objectifs (sensibilit√© prioritaire).\n",
    "- La d√©cision finale doit √™tre align√©e avec la capacit√© op√©rationnelle √† traiter les alertes g√©n√©r√©es.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77118f1f-2352-415b-9a9a-44381e7ef466",
   "metadata": {},
   "source": [
    "## Evaluation des performances finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb4a45-f06d-41cb-9ca5-7e20e839ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer les pr√©dictions finales avec le seuil optimal\n",
    "y_pred_final = (y_scores_test >= best_threshold).astype(int)\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(\"Matrice de confusion :\")\n",
    "print(cm)\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred_final,\n",
    "    target_names=[\"Non-d√©part\", \"D√©part\"],\n",
    "    digits=3\n",
    ")\n",
    "print(\"Rapport de classification :\")\n",
    "print(report)\n",
    "\n",
    "f2 = fbeta_score(y_test, y_pred_final, beta=2)\n",
    "print(f\"F2-score final : {f2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a521e-b267-48e9-889a-07598ae77582",
   "metadata": {},
   "source": [
    "##### Analyse de la matrice de confusion et du rapport de classification\n",
    "\n",
    "‚úÖ **Principaux constats :**\n",
    "- **Non-d√©part (classe majoritaire) :**\n",
    "  - Tr√®s bonne pr√©cision (**0,96**) : peu de faux positifs.\n",
    "  - Rappel plus faible (**0,66**) : environ un tiers des non-d√©parts sont mal class√©s.\n",
    "\n",
    "- **D√©part (classe minoritaire) :**\n",
    "  - Rappel √©lev√© (**0,87**) : la plupart des d√©parts sont d√©tect√©s.\n",
    "  - Pr√©cision faible (**0,33**) : beaucoup de faux positifs.\n",
    "  - F1-score modeste (**0,48**) : √©quilibre imparfait entre pr√©cision et rappel.\n",
    "\n",
    "‚úÖ **Performance globale :**\n",
    "- **Accuracy : 0,69** (mais peu informative en contexte d√©s√©quilibr√©).\n",
    "- **Macro moyenne F1 : 0,63** (moyenne non pond√©r√©e des classes).\n",
    "- **F2-score final : 0,655**, confirmant la priorit√© donn√©e au rappel.\n",
    "\n",
    "‚ö†Ô∏è **Points d‚Äôattention :**\n",
    "- La performance sur la classe \"D√©part\" d√©pend fortement du rappel, au d√©triment de la pr√©cision.\n",
    "- Le nombre √©lev√© de faux positifs (84) n√©cessitera une gestion attentive si le mod√®le est utilis√© en production.\n",
    "- Le choix du seuil refl√®te une strat√©gie assum√©e : **mieux vaut alerter que rater un d√©part**.\n",
    "\n",
    "**Synth√®se :**\n",
    "- Le mod√®le capte bien les d√©parts, avec un rappel satisfaisant.\n",
    "- La pr√©cision reste limit√©e, ce qui implique une communication claire sur le taux de fausses alertes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02945b-15bb-47fc-a1da-1d397a61b442",
   "metadata": {},
   "source": [
    "## Feature Importance Native XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5511c31-8c7c-4cd1-b72d-2e7c7e5f89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Extraire l'estimateur final depuis le pipeline optimis√©\n",
    "model_xgb = grid_search.best_estimator_.named_steps[\"modele_utilise\"]\n",
    "\n",
    "# Extraire les importances\n",
    "importances = model_xgb.feature_importances_\n",
    "\n",
    "# R√©cup√©rer les noms de colonnes finales avec une m√©thode robuste\n",
    "preprocessor = grid_search.best_estimator_.named_steps[\"preprocessing\"]\n",
    "\n",
    "feature_names = []\n",
    "\n",
    "for name, transformer, columns in preprocessor.transformers_:\n",
    "    # Ignore le 'drop'\n",
    "    if transformer == \"drop\":\n",
    "        continue\n",
    "    # Prend les colonnes telles quelles si passthrough\n",
    "    if transformer == \"passthrough\":\n",
    "        feature_names.extend(columns)\n",
    "        continue\n",
    "    # Cas d'un pipeline imbriqu√©\n",
    "    if hasattr(transformer, \"named_steps\"):\n",
    "        last_step = list(transformer.named_steps.values())[-1]\n",
    "        if hasattr(last_step, \"get_feature_names_out\"):\n",
    "            names = last_step.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "    else:\n",
    "        if hasattr(transformer, \"get_feature_names_out\"):\n",
    "            names = transformer.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "    feature_names.extend(names)\n",
    "\n",
    "# Cr√©er un DataFrame tri√©\n",
    "import pandas as pd\n",
    "\n",
    "df_importances = pd.DataFrame({\n",
    "    \"Variable\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Afficher le tableau\n",
    "print(df_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746c754-868a-443e-ae8d-3150831a6af6",
   "metadata": {},
   "source": [
    "##### Analyse importance des variables (importance native)\n",
    "\n",
    "- Les variables les plus importantes sont :\n",
    "  - **interaction_distance_heures_sup** et **interaction_satisfaction_anciennete** (~0,034).\n",
    "  - **age_revenu** et plusieurs variables li√©es au **poste occup√©** et au **d√©partement** (~0,03).\n",
    "- D‚Äôautres variables comportementales et de satisfaction apparaissent dans le haut du classement (**satisfaction_moyenne**, **surmenage_transports**).\n",
    "- Plusieurs variables ont une importance nulle, notamment :\n",
    "  - **heures_supplementaires_binaire**, certaines **tranches d‚Äô√¢ge**, et le **domaine d‚Äô√©tude RH**.\n",
    "- Ces r√©sultats refl√®tent surtout l‚Äôimpact structurel du mod√®le sur les splits des arbres.\n",
    "\n",
    "**Synth√®se :**\n",
    "Cette importance native oriente la compr√©hension des principaux facteurs, mais sera compl√©t√©e par l‚Äôanalyse par permutation plus robuste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0f35c-90ac-40a7-bec6-6c3125e49d79",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31871590-ede8-4716-9205-3ba98dc6971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer l'importance par permutation\n",
    "result = permutation_importance(\n",
    "    grid_search.best_estimator_,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    scoring=f2_scorer,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Utiliser directement les colonnes originales de X_test\n",
    "feature_names_perm = X_test.columns\n",
    "\n",
    "# V√©rification\n",
    "print(\"Nombre de colonnes X_test:\", len(feature_names_perm))\n",
    "print(\"Nombre d'importances:\", len(result.importances_mean))\n",
    "\n",
    "# Cr√©er le DataFrame\n",
    "importances_perm_df = pd.DataFrame({\n",
    "    \"Variable\": feature_names_perm,\n",
    "    \"Importance moyenne\": result.importances_mean,\n",
    "    \"√âcart-type\": result.importances_std\n",
    "}).sort_values(by=\"Importance moyenne\", ascending=False)\n",
    "\n",
    "# Afficher\n",
    "print(importances_perm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af8208-5ccb-4188-87e4-5bb7d24bf618",
   "metadata": {},
   "source": [
    "##### Synth√®se globale des importances (native et permutation)\n",
    "\n",
    "‚úÖ **Variables les plus contributives :**\n",
    "- Les deux m√©thodes confirment l‚Äôimportance de :\n",
    "  - **interaction_distance_heures_sup** (native : top 1, permutation : plus forte importance).\n",
    "  - **surmenage_transports** (importante en permutation, bien class√©e en native).\n",
    "- Ces variables traduisent le lien entre contraintes de d√©placement, surcharge et d√©part.\n",
    "\n",
    "‚úÖ **Diff√©rences notables entre m√©thodes :**\n",
    "- La permutation valorise beaucoup plus **nombre_participation_pee** et **tranche_age**.\n",
    "- Des variables importantes en importance native (ex. `age_revenu`, certaines modalit√©s de poste) apparaissent avec des importances n√©gatives ou nulles en permutation.\n",
    "  - Cela indique qu‚Äôelles structurent les splits mais apportent moins d‚Äôam√©lioration de la performance globale.\n",
    "\n",
    "‚úÖ **Variables avec importance nulle ou n√©gative :**\n",
    "- Plusieurs variables affichent des valeurs nulles ou n√©gatives en permutation :\n",
    "  - **revenu_stable**, **heures_supplementaires_binaire**, **satisfaction_employee**, etc.\n",
    "- Cela peut signifier qu‚Äôelles n‚Äôapportent pas d‚Äôinformation pr√©dictive apr√®s prise en compte des autres variables.\n",
    "\n",
    "**Synth√®se :**\n",
    "- L‚Äôimportance par permutation est g√©n√©ralement plus fiable pour juger la **vraie contribution pr√©dictive**.\n",
    "- La convergence sur certaines variables-cl√©s renforce leur cr√©dibilit√© (interactions et surcharge).\n",
    "- Les autres variables devront √™tre valid√©es ou √©cart√©es selon leur int√©r√™t m√©tier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdd39d-f2f0-4ddc-9197-dddebd96f5db",
   "metadata": {},
   "source": [
    "## Interpr√©tation SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ed920-c123-4be4-93e3-983ae454973b",
   "metadata": {},
   "source": [
    "### Importance globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca022f1e-c63a-4517-bca4-29c34ad44daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire le mod√®le XGBoost depuis le pipeline\n",
    "model_xgb = grid_search.best_estimator_.named_steps[\"modele_utilise\"]\n",
    "\n",
    "# Transformer X_test avec le preprocessor\n",
    "X_test_transformed = grid_search.best_estimator_.named_steps[\"preprocessing\"].transform(X_test)\n",
    "\n",
    "# Cr√©er l'explainer sp√©cifique √† XGBoost\n",
    "explainer = shap.TreeExplainer(model_xgb)\n",
    "\n",
    "# Calculer les valeurs SHAP\n",
    "shap_values = explainer.shap_values(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141860e1-e7c7-4d65-9a64-054a0690b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = grid_search.best_estimator_.named_steps[\"preprocessing\"]\n",
    "feature_names = []\n",
    "\n",
    "for name, transformer, columns in preprocessor.transformers_:\n",
    "    if transformer == \"drop\":\n",
    "        continue\n",
    "    if transformer == \"passthrough\":\n",
    "        feature_names.extend(columns)\n",
    "        continue\n",
    "    if hasattr(transformer, \"named_steps\"):\n",
    "        last_step = list(transformer.named_steps.values())[-1]\n",
    "        if hasattr(last_step, \"get_feature_names_out\"):\n",
    "            names = last_step.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "    else:\n",
    "        if hasattr(transformer, \"get_feature_names_out\"):\n",
    "            names = transformer.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "    feature_names.extend(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb4c02a-fca1-4c7a-874f-f689be1a2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier la forme des shap_values\n",
    "print(type(shap_values))\n",
    "if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "    # Pour la classe positive\n",
    "    shap_values_class1 = shap_values[1]\n",
    "else:\n",
    "    # Si shap_values est un seul array\n",
    "    shap_values_class1 = shap_values\n",
    "\n",
    "df_shap = pd.DataFrame(\n",
    "    shap_values_class1,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "# Aper√ßu des valeurs SHAP\n",
    "print(df_shap.head())\n",
    "\n",
    "df_shap.to_csv(\"shap_values_class1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95648f76-38d0-43e9-a2eb-93ac28bc3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap.abs().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ee0be-bc71-4826-a983-9bee55ca0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm plot\n",
    "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c95001-2cf9-407d-8c59-9683883078c6",
   "metadata": {},
   "source": [
    "##### Analyse des importances SHAP (valeurs absolues)\n",
    "\n",
    "‚úÖ **Variables les plus contributives :**\n",
    "- **interaction_distance_heures_sup** (0,456) est la variable avec l‚Äôimpact moyen le plus √©lev√©.\n",
    "- **nombre_participation_pee** (0,430) et **age_revenu** (0,370) confirment leur poids pr√©dictif important.\n",
    "- **satisfaction_moyenne** (0,343) et **interaction_satisfaction_anciennete** (0,314) occupent aussi une place cl√©.\n",
    "  - Ces variables traduisent des dimensions combin√©es de surcharge, stabilit√© et satisfaction.\n",
    "\n",
    "‚úÖ **Variables secondaires mais significatives :**\n",
    "- **taux_de_formation**, **revenu_mensuel**, et **frequence_deplacement** apportent une contribution notable (0,22‚Äì0,24).\n",
    "- Certaines modalit√©s de poste (**Assistant de Direction**, **Consultant**, **Cadre Commercial**) apparaissent √©galement avec des valeurs SHAP moyennes >0,07.\n",
    "\n",
    "‚úÖ **Variables √† impact faible ou nul :**\n",
    "- De nombreuses variables affichent des valeurs SHAP tr√®s basses (<0,02), voire nulles :\n",
    "  - **heures_supplementaires_binaire**, **tranches d‚Äô√¢ge Junior/Senior**, **Ressources Humaines**.\n",
    "  - Leur apport explicatif est n√©gligeable.\n",
    "\n",
    "**Synth√®se :**\n",
    "- L‚Äôanalyse SHAP compl√®te les autres m√©thodes (importance native, permutation) en confirmant le r√¥le dominant :\n",
    "  - Des variables d‚Äô**interaction et surcharge**.\n",
    "  - De la **satisfaction** et de l‚Äô**anciennet√© combin√©e au revenu**.\n",
    "- La convergence entre plusieurs approches renforce la confiance dans la s√©lection de ces variables comme facteurs cl√©s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c8e4a-f816-47c8-bd01-074feed82995",
   "metadata": {},
   "source": [
    "### Quelques exemples issues des deux classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30436c9d-0a38-49fe-a3b9-cecb64118e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les pr√©dictions finales avec le seuil optimal\n",
    "y_pred_final = (y_scores_test >= best_threshold).astype(int)\n",
    "\n",
    "# Cr√©er un DataFrame r√©capitulatif\n",
    "df_preds = pd.DataFrame({\n",
    "    \"R√©el\": y_test.values,\n",
    "    \"Pr√©dit\": y_pred_final,\n",
    "    \"Probabilit√©\": y_scores_test\n",
    "})\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(df_preds.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef734f-66d5-4bb7-8e42-1dd58d12dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation 4 : vrai positif\n",
    "shap.plots._waterfall.waterfall_legacy(\n",
    "    explainer.expected_value,\n",
    "    shap_values[4,:],\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faeb27b-d6f2-4dc5-9144-75cc8a9f27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation 0 : faux positif\n",
    "shap.plots._waterfall.waterfall_legacy(\n",
    "    explainer.expected_value,\n",
    "    shap_values[0,:],\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63479ec5-3e5b-4210-91d5-263b51aedca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation 1 : vrai n√©gatif\n",
    "shap.plots._waterfall.waterfall_legacy(\n",
    "    explainer.expected_value,\n",
    "    shap_values[1,:],\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdaa80f-b9e1-4fd5-a25a-dd9ed949d17b",
   "metadata": {},
   "source": [
    "# Synth√®se globale de la classification du turnover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcf5aa-00d4-4895-9ffd-dd455760cd4b",
   "metadata": {},
   "source": [
    "##### üéØ Synth√®se finale du projet\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Performances globales du mod√®le\n",
    "\n",
    "- **Mod√®le retenu :**\n",
    "  - XGBoostClassifier\n",
    "  - Hyperparam√®tres finaux :\n",
    "    - `max_depth=3`\n",
    "    - `learning_rate=0.1`\n",
    "    - `subsample=0.5`\n",
    "    - `scale_pos_weight=5`\n",
    "- **Seuil optimal choisi :** ‚âà0.24\n",
    "- **Scores en test :**\n",
    "  - Recall classe D√©part : **0.87**\n",
    "  - Precision classe D√©part : **0.33**\n",
    "  - F2-score final : **0.655**\n",
    "- **Interpr√©tation :**\n",
    "  - Le mod√®le privil√©gie la d√©tection des d√©parts (rappel √©lev√©).\n",
    "  - Il accepte un nombre important de faux positifs.\n",
    "  - Ce compromis est assum√© compte tenu de l‚Äôobjectif m√©tier.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Facteurs contributifs identifi√©s\n",
    "\n",
    "**1Ô∏è‚É£ Importance native**\n",
    "- Variables dominantes dans les arbres :\n",
    "  - `interaction_distance_heures_sup`\n",
    "  - `surmenage_transports`\n",
    "  - `age_revenu`\n",
    "- Ces variables structurent la construction des splits.\n",
    "\n",
    "**2Ô∏è‚É£ Importance par permutation**\n",
    "- Variables les plus contributives :\n",
    "  - `interaction_distance_heures_sup`\n",
    "  - `surmenage_transports`\n",
    "  - `nombre_participation_pee`\n",
    "- Certaines variables jug√©es importantes en native apparaissent faibles en permutation.\n",
    "\n",
    "**3Ô∏è‚É£ SHAP valeurs globales**\n",
    "- Variables √† impact moyen √©lev√© :\n",
    "  - `interaction_distance_heures_sup`\n",
    "  - `nombre_participation_pee`\n",
    "  - `satisfaction_moyenne`\n",
    "  - `age_revenu`\n",
    "- Les effets varient selon les individus.\n",
    "\n",
    "---\n",
    "\n",
    "##### üß≠ Interpr√©tation critique\n",
    "\n",
    "- Les trois m√©thodes convergent sur un constat :\n",
    "  **le mod√®le capte un signal dispers√©, sans variable unique d√©terminante.**\n",
    "- Ce constat est fr√©quent en RH :\n",
    "  - Les causes de d√©part sont multifactorielles.\n",
    "  - Certaines dimensions (climat, motivations personnelles) ne figurent pas dans le dataset.\n",
    "- Les corr√©lations sont faibles √† mod√©r√©es.\n",
    "\n",
    "---\n",
    "\n",
    "##### üß≠ Recommandations m√©tier prudentes\n",
    "\n",
    "1Ô∏è‚É£ **Surveiller les signaux faibles**\n",
    "   - Contraintes et surcharge (`interaction_distance_heures_sup`, `surmenage_transports`)\n",
    "   - Indicateurs de stabilit√© (`revenu_mensuel`, `age_revenu`)\n",
    "   - Participation aux dispositifs collectifs (`nombre_participation_pee`)\n",
    "\n",
    "2Ô∏è‚É£ **Compl√©ter l‚Äôanalyse par des donn√©es terrain**\n",
    "   - Entretiens de d√©part\n",
    "   - Enqu√™tes d‚Äôengagement\n",
    "\n",
    "3Ô∏è‚É£ **Renforcer le feature engineering**\n",
    "   - Int√©grer des √©volutions temporelles et des interactions suppl√©mentaires.\n",
    "\n",
    "4Ô∏è‚É£ **Ne pas surinterpr√©ter**\n",
    "   - Les pr√©dictions sont une aide √† la r√©flexion, non un jugement d√©finitif.\n",
    "\n",
    "---\n",
    "\n",
    "##### üìù Conclusion\n",
    "\n",
    "Le projet montre :\n",
    "- La faisabilit√© d‚Äôun mod√®le pr√©dictif reproductible.\n",
    "- La capacit√© √† identifier partiellement les profils √† risque.\n",
    "- La n√©cessit√© de combiner approche quantitative et vision qualitative.\n",
    "\n",
    "**En synth√®se :**\n",
    "> Ce mod√®le est un point de d√©part utile, mais doit √™tre compl√©t√© par une r√©flexion plus large sur les leviers d‚Äôengagement et de fid√©lisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e9f20-728e-454a-b21b-b93c82c3a338",
   "metadata": {},
   "source": [
    "## Export du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4e2dc-2be1-45cb-9a20-37df116107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export du pipeline\n",
    "joblib.dump(\n",
    "    grid_search.best_estimator_,\n",
    "    \"C:/Users/Fab/Documents/P5_D√©ployez_un_mod√®le_de_Machine_Learning/app/models/pipeline.joblib\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fa0f7-ab70-42d3-981d-3787ec191dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
