{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c476a20-a706-4dc6-b1a2-312256ee6cef",
   "metadata": {},
   "source": [
    "# Workflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840655c-b6a4-4b5e-8f3b-ca1a53cc04fa",
   "metadata": {},
   "source": [
    "## Import des modules nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77b5cdf-96c5-4d69-9bf8-c3ec1938b4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fab\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/Fab/Documents/P5_Déployez_un_modèle_de_Machine_Learning\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, fbeta_score, confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Métriques pour la classification\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    fbeta_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# SMOTE\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from app.utils.binary_mapper import BinaryMapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca385e2f-2af8-41d9-9402-e7e0408e04e8",
   "metadata": {},
   "source": [
    "## Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c3d71c-4793-4ccd-a9a3-af09a268dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employes_pipe = pd.read_csv('data/employes_net.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717dbd49-8aed-48d7-bb15-a28f1adb07a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 43 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   age                                        1470 non-null   int64  \n",
      " 1   genre                                      1470 non-null   object \n",
      " 2   revenu_mensuel                             1470 non-null   int64  \n",
      " 3   statut_marital                             1470 non-null   object \n",
      " 4   departement                                1470 non-null   object \n",
      " 5   poste                                      1470 non-null   object \n",
      " 6   nombre_experiences_precedentes             1470 non-null   int64  \n",
      " 7   annee_experience_totale                    1470 non-null   int64  \n",
      " 8   annees_dans_l_entreprise                   1470 non-null   int64  \n",
      " 9   annees_dans_le_poste_actuel                1470 non-null   int64  \n",
      " 10  a_quitte_l_entreprise                      1470 non-null   int64  \n",
      " 11  nombre_participation_pee                   1470 non-null   int64  \n",
      " 12  nb_formations_suivies                      1470 non-null   int64  \n",
      " 13  distance_domicile_travail                  1470 non-null   int64  \n",
      " 14  niveau_education                           1470 non-null   int64  \n",
      " 15  domaine_etude                              1470 non-null   object \n",
      " 16  frequence_deplacement                      1470 non-null   object \n",
      " 17  annees_depuis_la_derniere_promotion        1470 non-null   int64  \n",
      " 18  annes_sous_responsable_actuel              1470 non-null   int64  \n",
      " 19  satisfaction_employee_environnement        1470 non-null   int64  \n",
      " 20  note_evaluation_precedente                 1470 non-null   int64  \n",
      " 21  niveau_hierarchique_poste                  1470 non-null   int64  \n",
      " 22  satisfaction_employee_nature_travail       1470 non-null   int64  \n",
      " 23  satisfaction_employee_equipe               1470 non-null   int64  \n",
      " 24  satisfaction_employee_equilibre_pro_perso  1470 non-null   int64  \n",
      " 25  note_evaluation_actuelle                   1470 non-null   int64  \n",
      " 26  heures_supplementaires                     1470 non-null   object \n",
      " 27  augmentation_salaire_precedente            1470 non-null   float64\n",
      " 28  distance_domicile_travail_qcut             1470 non-null   object \n",
      " 29  tranche_age                                1470 non-null   object \n",
      " 30  ratio_stagnation                           1470 non-null   float64\n",
      " 31  ratio_sous_responsable                     1470 non-null   float64\n",
      " 32  age_revenu                                 1470 non-null   int64  \n",
      " 33  satisfaction_moyenne                       1470 non-null   float64\n",
      " 34  interaction_satisfaction_anciennete        1470 non-null   float64\n",
      " 35  taux_de_formation                          1470 non-null   float64\n",
      " 36  heures_supplementaires_binaire             1470 non-null   int64  \n",
      " 37  interaction_distance_heures_sup            1470 non-null   int64  \n",
      " 38  delta_evaluation                           1470 non-null   int64  \n",
      " 39  revenu_stable                              1470 non-null   int64  \n",
      " 40  delta_eval_x_revenu_stable                 1470 non-null   int64  \n",
      " 41  frequence_deplacement_num                  1470 non-null   int64  \n",
      " 42  surmenage_transports                       1470 non-null   int64  \n",
      "dtypes: float64(6), int64(28), object(9)\n",
      "memory usage: 494.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_employes_pipe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0627bfa8-aea3-42b1-9540-ce12a904a6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_employes_pipe[\"revenu_quartile\"] = df_employes_pipe[\"revenu_quartile\"].astype(\"category\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_employes_pipe[\"distance_domicile_travail_qcut\"] = df_employes_pipe[\"distance_domicile_travail_qcut\"].astype(\"category\")\n",
    "'''df_employes_pipe[\"revenu_quartile\"] = df_employes_pipe[\"revenu_quartile\"].astype(\"category\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003d04d-ee47-4166-8461-d78b98bdd830",
   "metadata": {},
   "source": [
    "## Sélection des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f83db7-f2fd-43ca-8ff4-400d5e94935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables numériques continues – MinMaxScaler\n",
    "# Nouvelle liste num_MinMax_vars incluant toutes les colonnes numériques continues\n",
    "num_MinMax_vars = [\n",
    "    \"revenu_mensuel\",\n",
    "    \"age\",\n",
    "    \"annee_experience_totale\",\n",
    "    \"annees_dans_l_entreprise\",\n",
    "    \"annees_dans_le_poste_actuel\",\n",
    "    \"annes_sous_responsable_actuel\",\n",
    "    \"nombre_participation_pee\",\n",
    "    \"ratio_stagnation\",\n",
    "    \"ratio_sous_responsable\",\n",
    "    \"age_revenu\",\n",
    "    \"satisfaction_moyenne\",\n",
    "    \"interaction_satisfaction_anciennete\",\n",
    "    \"taux_de_formation\",\n",
    "    \"interaction_distance_heures_sup\",\n",
    "    \"surmenage_transports\",\n",
    "    # Les 10 colonnes supplémentaires\n",
    "    \"revenu_stable\",\n",
    "    \"distance_domicile_travail\",\n",
    "    \"nb_formations_suivies\",\n",
    "    \"delta_eval_x_revenu_stable\",\n",
    "    \"delta_evaluation\",\n",
    "    \"nombre_experiences_precedentes\",\n",
    "    \"annees_depuis_la_derniere_promotion\",\n",
    "    \"frequence_deplacement_num\",\n",
    "    \"augmentation_salaire_precedente\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9eb929f-7e7f-46ff-a43d-efbc6c991925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables catégorielles nominales - OneHotEncoder\n",
    "cat_nominal_onehot_vars = [\n",
    "    \"statut_marital\",\n",
    "    \"departement\",\n",
    "    \"poste\",\n",
    "    \"domaine_etude\",\n",
    "    \"distance_domicile_travail_qcut\",\n",
    "    \"tranche_age\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c2d2b3-c19c-4fca-b66f-eee2016e1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables catégorielles ordinales numériques – OrdinalEncoder\n",
    "cat_ordinal_numeric_vars = [\n",
    "    \"niveau_education\",\n",
    "    \"satisfaction_employee_environnement\",\n",
    "    \"note_evaluation_precedente\",\n",
    "    \"niveau_hierarchique_poste\",\n",
    "    \"satisfaction_employee_nature_travail\",\n",
    "    \"satisfaction_employee_equipe\",\n",
    "    \"satisfaction_employee_equilibre_pro_perso\",\n",
    "    \"note_evaluation_actuelle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58605eb3-e313-4ca4-a4d5-09b7a29794f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat_ordinal_texte_vars = [\"frequence_deplacement\", \"revenu_quartile\"]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables catégorielles ordinales texte – OrdinalEncoder\n",
    "cat_ordinal_texte_vars = [\"frequence_deplacement\"]\n",
    "'''cat_ordinal_texte_vars = [\"frequence_deplacement\", \"revenu_quartile\"]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4ab0f5-8bf1-47ff-a13a-baf4726fe803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a ransformer en binaire (0/1)\n",
    "binary_mapping = {\n",
    "    \"genre\": {\"M\": 1, \"F\": 0},\n",
    "    \"heures_supplementaires\": {\"Oui\": 1, \"Non\": 0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09acfa44-2f7a-41ec-a374-94f546c6f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables déjà binaires \n",
    "binary_vars = [\n",
    "    \"heures_supplementaires_binaire\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc34af5b-ac35-4158-a837-6d8145f810d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable cible\n",
    "target_var = \"a_quitte_l_entreprise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af7c6f3e-df35-44a2-b9da-ae9dec7f14d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 43 columns):\n",
      " #   Column                                     Non-Null Count  Dtype   \n",
      "---  ------                                     --------------  -----   \n",
      " 0   age                                        1470 non-null   int64   \n",
      " 1   genre                                      1470 non-null   object  \n",
      " 2   revenu_mensuel                             1470 non-null   int64   \n",
      " 3   statut_marital                             1470 non-null   object  \n",
      " 4   departement                                1470 non-null   object  \n",
      " 5   poste                                      1470 non-null   object  \n",
      " 6   nombre_experiences_precedentes             1470 non-null   int64   \n",
      " 7   annee_experience_totale                    1470 non-null   int64   \n",
      " 8   annees_dans_l_entreprise                   1470 non-null   int64   \n",
      " 9   annees_dans_le_poste_actuel                1470 non-null   int64   \n",
      " 10  a_quitte_l_entreprise                      1470 non-null   int64   \n",
      " 11  nombre_participation_pee                   1470 non-null   int64   \n",
      " 12  nb_formations_suivies                      1470 non-null   int64   \n",
      " 13  distance_domicile_travail                  1470 non-null   int64   \n",
      " 14  niveau_education                           1470 non-null   int64   \n",
      " 15  domaine_etude                              1470 non-null   object  \n",
      " 16  frequence_deplacement                      1470 non-null   object  \n",
      " 17  annees_depuis_la_derniere_promotion        1470 non-null   int64   \n",
      " 18  annes_sous_responsable_actuel              1470 non-null   int64   \n",
      " 19  satisfaction_employee_environnement        1470 non-null   int64   \n",
      " 20  note_evaluation_precedente                 1470 non-null   int64   \n",
      " 21  niveau_hierarchique_poste                  1470 non-null   int64   \n",
      " 22  satisfaction_employee_nature_travail       1470 non-null   int64   \n",
      " 23  satisfaction_employee_equipe               1470 non-null   int64   \n",
      " 24  satisfaction_employee_equilibre_pro_perso  1470 non-null   int64   \n",
      " 25  note_evaluation_actuelle                   1470 non-null   int64   \n",
      " 26  heures_supplementaires                     1470 non-null   object  \n",
      " 27  augmentation_salaire_precedente            1470 non-null   float64 \n",
      " 28  distance_domicile_travail_qcut             1470 non-null   category\n",
      " 29  tranche_age                                1470 non-null   object  \n",
      " 30  ratio_stagnation                           1470 non-null   float64 \n",
      " 31  ratio_sous_responsable                     1470 non-null   float64 \n",
      " 32  age_revenu                                 1470 non-null   int64   \n",
      " 33  satisfaction_moyenne                       1470 non-null   float64 \n",
      " 34  interaction_satisfaction_anciennete        1470 non-null   float64 \n",
      " 35  taux_de_formation                          1470 non-null   float64 \n",
      " 36  heures_supplementaires_binaire             1470 non-null   int64   \n",
      " 37  interaction_distance_heures_sup            1470 non-null   int64   \n",
      " 38  delta_evaluation                           1470 non-null   int64   \n",
      " 39  revenu_stable                              1470 non-null   int64   \n",
      " 40  delta_eval_x_revenu_stable                 1470 non-null   int64   \n",
      " 41  frequence_deplacement_num                  1470 non-null   int64   \n",
      " 42  surmenage_transports                       1470 non-null   int64   \n",
      "dtypes: category(1), float64(6), int64(28), object(8)\n",
      "memory usage: 484.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_employes_pipe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cea1fa-86b9-4cd0-b594-0e76e2786009",
   "metadata": {},
   "source": [
    "## Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09cf91d7-432d-4943-a761-ba1205080c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_employes_pipe.drop(columns=[target_var])\n",
    "y = df_employes_pipe[target_var]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02559f05-4d68-4610-8e6f-bf6fbaaea617",
   "metadata": {},
   "source": [
    "## Définition des sous-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fff9fd7b-7518-45c0-8fee-8c6c5bc01fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Création d'une classe de transformation pour le genre\\nclass BinaryMapper(BaseEstimator, TransformerMixin):\\n    def __init__(self, mapping=None):\\n        # mapping doit être un dict : {colonne: {valeur: code, ...}, ...}\\n        self.mapping = mapping or {} \\n\\n    def fit(self, X, y=None):\\n        return self  # pas d'apprentissage nécessaire\\n\\n    def transform(self, X):\\n        X_ = X.copy()\\n        for col, col_map in self.mapping.items():\\n            X_[col] = X_[col].map(col_map)\\n        return X_\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Création d'une classe de transformation pour le genre\n",
    "class BinaryMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mapping=None):\n",
    "        # mapping doit être un dict : {colonne: {valeur: code, ...}, ...}\n",
    "        self.mapping = mapping or {} \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # pas d'apprentissage nécessaire\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for col, col_map in self.mapping.items():\n",
    "            X_[col] = X_[col].map(col_map)\n",
    "        return X_'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c18d57-00cc-4659-a015-a5a4268f9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On déclare les valeurs binaires pour le genre :\n",
    "binary_mapping = {\n",
    "    \"genre\": {\"M\": 1, \"F\": 0},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6857da74-10f4-41a7-bfe3-0416ee5e0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sous-pipeline de normalisation\n",
    "normalisation_pipeline = Pipeline([\n",
    "    ('minmax', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Sous-pipeline de OneHotEncoder\n",
    "hot_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Sous-pipeline OrdinalEncoder sur variables numériques ordonnées\n",
    "ordinal_num_pipeline = Pipeline([\n",
    "    ('ordinal_num', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# Sous-pipeline OrdinalEncoder sur variables textuelles explicitement ordonnées\n",
    "ordinal_texte_pipeline = Pipeline([\n",
    "    ('ordinal_texte', OrdinalEncoder(categories=[\n",
    "        ['Aucun', 'Occasionnel', 'Frequent'],\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Sous-pipeline de binarisation du genre uniquement\n",
    "binary_pipeline = Pipeline([\n",
    "    ('binariseur', BinaryMapper(mapping=binary_mapping))\n",
    "])\n",
    "\n",
    "# Sous-pipeline \n",
    "binary_pass_pipeline = Pipeline([\n",
    "    ('pass_through', FunctionTransformer(validate=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc56b89-92e0-4987-bd5f-075f95779847",
   "metadata": {},
   "source": [
    "## Assemblage avec ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e8726b-e3dd-42ec-af8c-73cbd3244b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemblage\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('normal', normalisation_pipeline, num_MinMax_vars),\n",
    "    ('hot', hot_pipeline, cat_nominal_onehot_vars),\n",
    "    ('ordinal_num', ordinal_num_pipeline, cat_ordinal_numeric_vars),\n",
    "    ('ordinal_texte', ordinal_texte_pipeline, cat_ordinal_texte_vars),\n",
    "    ('binary_mapper', binary_pipeline, [\"genre\", \"heures_supplementaires\"]),\n",
    "    ('binary_pass', binary_pass_pipeline, binary_vars)\n",
    "],\n",
    "                                 remainder=\"drop\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485f5bf-fd93-4118-b76b-88a7fbf578d0",
   "metadata": {},
   "source": [
    "## Pipeline complet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a4230-cbdd-4f3c-a023-997b729172f7",
   "metadata": {},
   "source": [
    "### Sélection des métriques et fonction d'affichage des valeurs de métriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92094bd-48f6-4b83-8cc0-8f3593366f01",
   "metadata": {},
   "source": [
    "### Choix des métriques de classification\n",
    "\n",
    "- **Accuracy** : Pourcentage de prédictions correctes globalement.\n",
    "- **Précision** : Parmi les employés prédits comme partants, combien le sont réellement. Important si faux positifs coûteux.\n",
    "- **Rappel** : Parmi les employés partis, combien sont bien détectés. Important si faux négatifs coûteux.\n",
    "- **F-beta score** : Combine précision et rappel avec un poids ajustable selon la priorité (par défaut beta=1 = F1).\n",
    "- **Matrice de confusion** : Pour voir le détail des erreurs (Faux Positifs, Faux Négatifs).\n",
    "- **Classification report** : Pour une synthèse complète des métriques par classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcfe7db3-c82b-4195-b504-9c424233ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratifiedKFold avec 5 splits\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb0bcb5d-39b3-4fad-827f-f8465f2438c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les scores par modèle\n",
    "resultats_cv = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fae84-dec8-462d-a7d9-6892c6aff4a9",
   "metadata": {},
   "source": [
    "***⚙️ Démarche de comparaison \"out of the box\" et validation croisée***\n",
    "\n",
    "Avant d'engager une optimisation approfondie des hyperparamètres (Grid Search), nous allons comparer plusieurs modèles de classification \"out of the box\".  \n",
    "Cela signifie que nous les utilisons **avec leurs paramètres par défaut**, sans réglage spécifique.  \n",
    "\n",
    "L'objectif est de :\n",
    "- Obtenir une **première mesure neutre de performance**, pour chaque algorithme.\n",
    "- Identifier rapidement les modèles les plus prometteurs.\n",
    "- Vérifier la stabilité des performances via une validation croisée.\n",
    "\n",
    "Nous avons ainsi mis en place une **validation croisée à 5 folds** sur les données d'entraînement.  \n",
    "Pour chaque modèle :\n",
    "- Les données sont découpées en 5 sous-ensembles (folds).\n",
    "- À chaque itération, le modèle est entraîné sur 4 folds et évalué sur le fold restant.\n",
    "- Nous calculons 4 métriques principales : Accuracy, Précision, Rappel et F2-score.\n",
    "- Nous stockons la moyenne et l'écart-type de chaque métrique.\n",
    "\n",
    "Cette approche progressive nous permet :\n",
    "- De comparer objectivement les modèles sur une base identique.\n",
    "- De repérer ceux qui présentent un potentiel avant le fine-tuning.\n",
    "- D'éviter de passer du temps à optimiser un modèle qui serait déjà en difficulté dans sa version standard.\n",
    "\n",
    "En résumé, la validation croisée \"out of the box\" constitue un **premier filtre exploratoire** avant de lancer la recherche approfondie des meilleurs hyperparamètres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5940014a-fbd2-4bee-90b9-03172d3f1bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Validation croisée pour le modèle : Modèle Dummy ====\n",
      "\n",
      "Fold 1/5\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "==== Validation croisée pour le modèle : Random Forest Classifier ====\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Non'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m--------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     36\u001b[39m pipeline_cv = Pipeline([\n\u001b[32m     37\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m     38\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mmodele_utilise\u001b[39m\u001b[33m'\u001b[39m, modele)\n\u001b[32m     39\u001b[39m ])\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Entraîner\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mpipeline_cv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fold_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Prédire (seuil par défaut 0.5)\u001b[39;00m\n\u001b[32m     45\u001b[39m y_pred = pipeline_cv.predict(X_fold_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\imblearn\\pipeline.py:526\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    521\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    522\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    523\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    524\u001b[39m             all_params=params,\n\u001b[32m    525\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:360\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[32m    373\u001b[39m estimator = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator)(criterion=\u001b[38;5;28mself\u001b[39m.criterion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1053\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1058\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\p5-déployez-un-modèle-de-machine-learning-c3yHBvQq-py3.13\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    837\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Non'"
     ]
    }
   ],
   "source": [
    "modeles = {\n",
    "    \"Modèle Dummy\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(random_state=42),\n",
    "    \"Regression Logistique\": LogisticRegression(random_state=42),\n",
    "    \"XGBoost Classifier\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"CatBoost Classifier\": CatBoostClassifier(\n",
    "        verbose=0,   # Pas d'affichage des logs pendant le fit. CatBoost affiche par défaut beaucoup de logs.Ici on les désactive pour que la boucle reste lisible.\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "for nom_modele, modele in modeles.items():\n",
    "    print(f\"\\n==== Validation croisée pour le modèle : {nom_modele} ====\")\n",
    "    \n",
    "    # Initialiser les listes pour stocker les scores\n",
    "    scores_accuracy = []\n",
    "    scores_precision = []\n",
    "    scores_recall = []\n",
    "    scores_f2 = []\n",
    "    \n",
    "    # Boucle sur les folds\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(f\"\\nFold {fold+1}/5\")\n",
    "        \n",
    "        # Séparer les données\n",
    "        X_fold_train = X_train.iloc[train_idx]\n",
    "        y_fold_train = y_train.iloc[train_idx]\n",
    "        X_fold_val = X_train.iloc[val_idx]\n",
    "        y_fold_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        # Créer un pipeline propre à chaque fold        \n",
    "        pipeline_cv = Pipeline([\n",
    "            ('preprocessing', preprocessor),\n",
    "            ('modele_utilise', modele)\n",
    "        ])\n",
    "                             \n",
    "        # Entraîner\n",
    "        pipeline_cv.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Prédire (seuil par défaut 0.5)\n",
    "        y_pred = pipeline_cv.predict(X_fold_val)\n",
    "        \n",
    "        # Calculer les métriques\n",
    "        acc = accuracy_score(y_fold_val, y_pred)\n",
    "        prec = precision_score(y_fold_val, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_fold_val, y_pred, zero_division=0)\n",
    "        f2 = fbeta_score(y_fold_val, y_pred, beta=2, zero_division=0)\n",
    "        \n",
    "        # Stocker\n",
    "        scores_accuracy.append(acc)\n",
    "        scores_precision.append(prec)\n",
    "        scores_recall.append(rec)\n",
    "        scores_f2.append(f2)\n",
    "    \n",
    "    # Résumer\n",
    "    resultats_cv[nom_modele] = {\n",
    "        'Accuracy': (np.mean(scores_accuracy), np.std(scores_accuracy)),\n",
    "        'Precision': (np.mean(scores_precision), np.std(scores_precision)),\n",
    "        'Recall': (np.mean(scores_recall), np.std(scores_recall)),\n",
    "        'F2-score': (np.mean(scores_f2), np.std(scores_f2))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b052f3-0c30-40b5-bf07-d0dfc9bca29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour le dataframe\n",
    "tableau_resultats = []\n",
    "\n",
    "for modele, metriques in resultats_cv.items():\n",
    "    ligne = {\n",
    "        'Modèle': modele,\n",
    "        'Accuracy (moy ± std)': f\"{metriques['Accuracy'][0]:.3f} ± {metriques['Accuracy'][1]:.3f}\",\n",
    "        'Precision (moy ± std)': f\"{metriques['Precision'][0]:.3f} ± {metriques['Precision'][1]:.3f}\",\n",
    "        'Recall (moy ± std)': f\"{metriques['Recall'][0]:.3f} ± {metriques['Recall'][1]:.3f}\",\n",
    "        'F2-score (moy ± std)': f\"{metriques['F2-score'][0]:.3f} ± {metriques['F2-score'][1]:.3f}\"\n",
    "    }\n",
    "    tableau_resultats.append(ligne)\n",
    "\n",
    "df_resultats_cv = pd.DataFrame(tableau_resultats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d19b0-fbe9-40da-8c75-f56a6937ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultats_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415fa6e-cb25-4fa9-861c-cc90e20c7d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles et des métriques\n",
    "modeles = df_resultats_cv['Modèle'].tolist()\n",
    "metriques = ['Accuracy', 'Precision', 'Recall', 'F2-score']\n",
    "\n",
    "# Extraction des moyennes et écarts-types\n",
    "moyennes = {m: [] for m in metriques}\n",
    "ecarts = {m: [] for m in metriques}\n",
    "\n",
    "for m in metriques:\n",
    "    for i in range(len(df_resultats_cv)):\n",
    "        # La colonne s'appelle ex: \"Accuracy (moy ± std)\"\n",
    "        texte = df_resultats_cv.iloc[i][f\"{m} (moy ± std)\"]\n",
    "        moy, std = texte.split('±')\n",
    "        moyennes[m].append(float(moy.strip()))\n",
    "        ecarts[m].append(float(std.strip()))\n",
    "\n",
    "# Paramètres du graphique\n",
    "x = np.arange(len(modeles))  # positions des modèles\n",
    "largeur = 0.2  # largeur de chaque barre\n",
    "\n",
    "# Création de la figure\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Pour chaque métrique, créer une série de barres\n",
    "for idx, m in enumerate(metriques):\n",
    "    positions = x + (idx - 1.5)*largeur\n",
    "    plt.bar(\n",
    "        positions,\n",
    "        moyennes[m],\n",
    "        width=largeur,\n",
    "        yerr=ecarts[m],\n",
    "        capsize=4,\n",
    "        label=m\n",
    "    )\n",
    "\n",
    "plt.xticks(x, modeles, rotation=15)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel(\"Score moyen (cross-validation)\")\n",
    "plt.title(\"Comparaison des performances des modèles (moyennes ± écarts-types)\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464469c9-4dcf-404c-91c4-a9fe842adefb",
   "metadata": {},
   "source": [
    "Nous allons retenir **XGBoostClassifier** car il présente les meilleures performances parmi les modèles non-linéaires (exigence du projet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997cde8-fe4a-4216-9049-2770bf3c833f",
   "metadata": {},
   "source": [
    "## Optimisation des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e67c19-3a65-4ced-8b97-0c2dce63e02d",
   "metadata": {},
   "source": [
    "### Définition de la grille d'hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c394c-da1a-453a-97f8-2c77cf536904",
   "metadata": {},
   "source": [
    "### 🎯 Fine-tuning de XGBoostClassifier : choix des hyperparamètres\n",
    "\n",
    "Pour optimiser notre XGBoostClassifier, nous avons ciblé **quatre hyperparamètres prioritaires**, reconnus comme les plus influents dans la littérature et la pratique :\n",
    "\n",
    "- **max_depth** (profondeur maximale des arbres)  \n",
    "  Permet de moduler la complexité du modèle.  \n",
    "  Des valeurs plus élevées captent plus de détails, mais augmentent le risque de surapprentissage.\n",
    "  \n",
    "- **learning_rate** (alias *eta*, taux d’apprentissage)  \n",
    "  Contrôle l’amplitude des mises à jour du modèle.  \n",
    "  Un learning rate faible ralentit l’apprentissage mais favorise une convergence plus stable.\n",
    "  \n",
    "- **subsample**  \n",
    "  Fraction des échantillons utilisés par arbre.  \n",
    "  Introduit une forme de *bagging* qui réduit la variance et limite le surapprentissage.  \n",
    "  Le *bagging* consiste à entraîner plusieurs modèles sur des sous-échantillons différents, puis à agréger leurs prédictions, ce qui stabilise les résultats.\n",
    "  \n",
    "- **scale_pos_weight**  \n",
    "  Coefficient de pondération des observations de la classe minoritaire.  \n",
    "  Ce paramètre est particulièrement utile dans un contexte de classes déséquilibrées.  \n",
    "  Il permet d’indiquer à l’algorithme qu’une erreur sur la classe positive est plus pénalisante.  \n",
    "  Cela peut améliorer le rappel sans recourir systématiquement à l’oversampling.\n",
    "  \n",
    "Trois de ces quatre paramètres sont mis en avant comme le **\"trio clé\"** à optimiser en priorité, notamment dans cet [Article source](https://medium.com/%40nekhumbecolbert3/unleashing-the-power-of-catboostclassifier-a-robust-model-for-categorical-feature-handling-a05e7dcf23b2) (Unleashing the Power of CatBoostClassifier: A Robust Model for Categorical Feature Handling de Humbulani Colbert, Medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e60ff6-a7fe-4273-a77b-63539aa77985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des hyperparamètres\n",
    "param_grid_xgboost = {\n",
    "    'modele_utilise__max_depth': [3, 6, 9],\n",
    "    'modele_utilise__learning_rate': [0.1, 0.3, 0.5],\n",
    "    'modele_utilise__subsample': [0.25, 0.5, 1.0],\n",
    "    'modele_utilise__scale_pos_weight': [1, 3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dffde6-1e11-4df8-ab34-87be354ee0ba",
   "metadata": {},
   "source": [
    "##### 🔍 Raisonnement derrière les choix de valeurs\n",
    "\n",
    "Nous avons appliqué une méthode raisonnée consistant à explorer, pour chaque hyperparamètre, **la valeur par défaut et deux variantes contrastées** (une plus prudente, une plus ambitieuse).  \n",
    "Cette approche permet d’identifier l’influence des réglages sans partir dans un grid trop vaste.\n",
    "\n",
    "---\n",
    "\n",
    "##### 📊 Hyperparamètres de complexité et régularisation\n",
    "\n",
    "| Hyperparamètre    | Valeur par défaut      | Variante “-1”              | Variante “+1”               |\n",
    "|-------------------|------------------------|----------------------------|-----------------------------|\n",
    "| **max_depth**     | 6                      | 3 (modèle moins complexe)  | 9 (modèle plus complexe)    |\n",
    "| **learning_rate** | 0.3                    | 0.1 (apprentissage plus régulier) | 0.5 (apprentissage plus rapide) |\n",
    "| **subsample**     | 1.0                    | 0.5 (bagging plus fort)    | 1.0 (bagging complet)       |\n",
    "\n",
    "Ces valeurs sont directement inspirées des recommandations courantes pour **maîtriser le compromis biais/variance** et tester l’impact de la profondeur et de la vitesse d’apprentissage.\n",
    "\n",
    "---\n",
    "\n",
    "##### ⚖️ Hyperparamètre de pondération des classes\n",
    "\n",
    "| Hyperparamètre         | Valeur par défaut | Variante “+1”          | Variante “+2”           |\n",
    "|------------------------|-------------------|------------------------|-------------------------|\n",
    "| **scale_pos_weight**   | 1                 | 3 (pondération modérée) | 5 (pondération plus forte) |\n",
    "\n",
    "**Pourquoi cet encadrement différent ?**\n",
    "\n",
    "- Contrairement aux autres hyperparamètres, `scale_pos_weight` **ne régule pas la complexité**, mais **rééquilibre la pénalisation des classes dans la fonction de perte**.\n",
    "- Les bonnes pratiques recommandent de tester plusieurs valeurs supérieures à 1 lorsque la classe positive est minoritaire.\n",
    "- Même si notre taux de départ est ≈16 % (et non ultra-déséquilibré), il était pertinent d’examiner si une pondération pouvait améliorer le rappel.\n",
    "- Nous avons volontairement choisi des valeurs **uniquement supérieures à la référence (1)**, car la pondération inférieure n’a pas de sens pratique ici.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**\n",
    "Cette structuration permet de :\n",
    "- Conserver un grid limité et rationnel.\n",
    "- Séparer clairement les paramètres qui contrôlent la complexité et ceux qui agissent sur le déséquilibre.\n",
    "- Justifier en entretien que chaque hyperparamètre a été réfléchi en fonction de son rôle spécifique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1eac7d-7710-4a90-9699-8e30039745a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un scorer personnalisé qui calcule le F2-score (pondère davantage le rappel) à chaque fold\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Pipeline complet\n",
    "# ‼️1er essai scale_pos_weight sans SMOTE\n",
    "pipeline_XGB_no_smote = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('modele_utilise', XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_XGB_no_smote,\n",
    "    param_grid=param_grid_xgboost,\n",
    "    scoring=f2_scorer,\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Lancement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Résultats\n",
    "print(\"✅ Meilleurs hyperparamètres trouvés :\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\n✅ Meilleur F2-score moyen (CV) :\")\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75c9ba-747e-4d2b-b345-8cf56dc89c89",
   "metadata": {},
   "source": [
    "##### 🔍 Résultats du Grid Search\n",
    "\n",
    "**Synthèse des hyperparamètres retenus :**\n",
    "\n",
    "- `learning_rate = 0.1`  \n",
    "  Le taux d’apprentissage le plus faible testé.  \n",
    "  \n",
    "- `max_depth = 3`  \n",
    "  La profondeur minimale parmi les valeurs testées.  \n",
    "  Cela montre qu’un modèle peu complexe suffit à capturer les patterns principaux, tout en limitant le risque de surapprentissage.\n",
    "  \n",
    "- `subsample = 0.25`  \n",
    "  La fraction la plus réduite des échantillons par arbre.  \n",
    "  Ce bagging fort introduit davantage de diversité entre les arbres.\n",
    "  \n",
    "- `scale_pos_weight = 5`  \n",
    "  La pondération maximale explorée pour la classe minoritaire.  \n",
    "  Cela confirme que renforcer l’attention portée aux départs améliore le rappel, ce qui est cohérent avec l’objectif métier.\n",
    "\n",
    "**Performance obtenue :**\n",
    "\n",
    "- Le **F2-score moyen sur validation croisée est de 0.534**, ce qui constitue une amélioration significative par rapport à la plupart des configurations “out of the box”.\n",
    "- Ce score reflète un bon compromis rappel/précision, avec un accent mis sur le rappel comme souhaité.\n",
    "\n",
    "**Interprétation :**\n",
    "\n",
    "- Ces réglages illustrent qu’en contexte de déséquilibre de classes, il est souvent préférable :\n",
    "  - De privilégier des modèles simples et réguliers (`max_depth` faible, `learning_rate` bas).\n",
    "  - D’ajouter une forte pondération des observations minoritaires (`scale_pos_weight` élevé).\n",
    "  - De renforcer la diversité par le sous-échantillonnage (`subsample` faible).\n",
    "\n",
    "En résumé, le Grid Search a permis de stabiliser les performances et d’identifier des hyperparamètres cohérents avec les contraintes du projet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681777d-bd2d-4d51-9c1a-fce3bddeef99",
   "metadata": {},
   "source": [
    "## Optimisation du seuil de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994feb4f-17cd-4a4c-9c44-d1c923dfea18",
   "metadata": {},
   "source": [
    "### Courbe ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfc1e3-18eb-41dc-9e28-fe5c68b0bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les probabilités de la classe positive\n",
    "y_scores_test = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculer la courbe ROC\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_scores_test)\n",
    "\n",
    "# Calculer l'AUC de la ROC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Tracer la courbe ROC\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", color=\"grey\", label=\"Random classifier\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02868483-2a52-4938-859e-41c888eb56ec",
   "metadata": {},
   "source": [
    "##### Analyse de la courbe ROC\n",
    "\n",
    "La courbe ROC présentée ci-dessus mesure la capacité du modèle à distinguer les classes **départ** et **non-départ**.  \n",
    "\n",
    "✅ **Principaux constats :**\n",
    "- La courbe ROC s’éloigne bien de la diagonale aléatoire, indiquant que le modèle a une capacité discriminante.\n",
    "- La **surface sous la courbe (AUC) est de 0,79**, ce qui reflète une performance globale correcte.\n",
    "\n",
    "⚠️ **Éléments de prudence :**\n",
    "- La courbe ROC n’indique pas si le modèle est meilleur pour limiter les **faux positifs** (prédire un départ à tort) ou les **faux négatifs** (ne pas détecter un départ).\n",
    "- Comme les données sont déséquilibrées (beaucoup moins de départs que de non-départs), la ROC peut donner une impression **trop optimiste de la qualité prédictive**.\n",
    "  \n",
    "➡️ **Prochaines étapes :**\n",
    "C’est pourquoi nous allons maintenant analyser la **courbe Précision–Rappel**, plus adaptée à l’évaluation des modèles sur des jeux de données déséquilibrés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e0b38-7c59-4cee-874f-1bc35239f5bc",
   "metadata": {},
   "source": [
    "### Courbes Précision-Rappel et F2-score vs seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859c7b0-195d-49fc-ac9b-20e24eb9a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les probabilités de la classe positive\n",
    "y_scores_test = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores_test)\n",
    "\n",
    "# Calculer le F2-score pour chaque seuil\n",
    "f2_scores = []\n",
    "for thresh in thresholds:\n",
    "    preds = (y_scores_test >= thresh).astype(int)\n",
    "    score = fbeta_score(y_test, preds, beta=2)\n",
    "    f2_scores.append(score)\n",
    "\n",
    "# Trouver le seuil optimal\n",
    "best_idx = np.argmax(f2_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f2 = f2_scores[best_idx]\n",
    "\n",
    "print(f\"Seuil optimal : {best_threshold:.4f}\")\n",
    "print(f\"Meilleur F2-score : {best_f2:.4f}\")\n",
    "\n",
    "# Tracer la courbe Précision–Rappel\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve - Jeu de Test\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Tracer la courbe F2-score en fonction du seuil\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(thresholds, f2_scores, label=\"F2-score by Threshold\")\n",
    "plt.axvline(best_threshold, color='red', linestyle='--', label=f\"Best threshold = {best_threshold:.2f}\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F2-score\")\n",
    "plt.title(\"F2-score vs Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844e6a3-1dcb-4526-ade0-72aaaad78544",
   "metadata": {},
   "source": [
    "##### Analyse de la courbe Précision–Rappel\n",
    "\n",
    "La courbe Précision–Rappel permet d’évaluer la performance du modèle sur un jeu de données **déséquilibré**, en se concentrant sur la capacité à détecter les départs.\n",
    "\n",
    "✅ **Principaux constats :**\n",
    "- La précision diminue progressivement lorsque le rappel augmente, ce qui est un comportement attendu.\n",
    "- Pour des valeurs de rappel faibles (<0,2), la précision peut dépasser **0,8**, traduisant que lorsqu’on détecte peu de départs, ils sont souvent corrects.\n",
    "- Lorsque le rappel dépasse **0,5**, la précision chute autour de **0,4**, ce qui signifie que plus on cherche à capter de cas positifs, plus on génère de faux positifs.\n",
    "\n",
    "⚠️ **Limites et interprétation :**\n",
    "- Cette courbe met en évidence un compromis clair entre **précision** (qualité des alertes) et **rappel** (couverture des départs).\n",
    "- Pour un usage opérationnel (ex. ciblage des salariés à risque), il sera crucial de choisir un seuil adapté en fonction de la priorité :\n",
    "  - **Minimiser les faux positifs** (éviter d’alerter inutilement)\n",
    "  - ou **maximiser le rappel** (ne rater aucun départ).\n",
    "\n",
    "**Synthèse :**\n",
    "- La performance est correcte mais montre que le modèle a des limites dès qu’on souhaite capturer un grand nombre de départs.\n",
    "- Cette analyse complète la ROC et fournit une vision plus réaliste dans le contexte des classes déséquilibrées.\n",
    "\n",
    "##### Analyse de la courbe F2-Score vs Seuil\n",
    "\n",
    "Cette courbe montre comment le **F2-score** évolue en fonction du seuil de décision appliqué aux probabilités prédites.\n",
    "\n",
    "✅ **Principaux constats :**\n",
    "- Le **F2-score maximal (~0,65)** est obtenu pour un seuil autour de **0,24** (ligne rouge).\n",
    "  - Ce seuil favorise le rappel par rapport à la précision, ce qui est cohérent avec l’utilisation du F2-score (pondère davantage le rappel).\n",
    "- Pour des seuils plus élevés (>0,4), le F2-score baisse progressivement jusqu’à atteindre des valeurs proches de zéro.\n",
    "  - Cela indique que fixer un seuil trop strict conduit à manquer beaucoup de départs.\n",
    "\n",
    "⚠️ **Interprétation et précautions :**\n",
    "- Le seuil optimal ici est bien **inférieur à 0,5**, ce qui montre que le modèle a tendance à produire des probabilités faibles même pour les cas positifs.\n",
    "- Ce seuil sera choisi en cohérence avec les priorités métier :\n",
    "  - **Maximiser le rappel** (ne pas rater de départs) tout en conservant une précision acceptable.\n",
    "- Un seuil bas entraînera un nombre plus élevé de faux positifs, qu’il faudra expliquer et justifier lors de la mise en production.\n",
    "\n",
    "**Synthèse :**\n",
    "- La courbe permet de sélectionner un seuil adapté aux objectifs (sensibilité prioritaire).\n",
    "- La décision finale doit être alignée avec la capacité opérationnelle à traiter les alertes générées.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77118f1f-2352-415b-9a9a-44381e7ef466",
   "metadata": {},
   "source": [
    "## Evaluation des performances finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb4a45-f06d-41cb-9ca5-7e20e839ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer les prédictions finales avec le seuil optimal\n",
    "y_pred_final = (y_scores_test >= best_threshold).astype(int)\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(\"Matrice de confusion :\")\n",
    "print(cm)\n",
    "\n",
    "# Afficher le rapport de classification\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred_final,\n",
    "    target_names=[\"Non-départ\", \"Départ\"],\n",
    "    digits=3\n",
    ")\n",
    "print(\"Rapport de classification :\")\n",
    "print(report)\n",
    "\n",
    "f2 = fbeta_score(y_test, y_pred_final, beta=2)\n",
    "print(f\"F2-score final : {f2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a521e-b267-48e9-889a-07598ae77582",
   "metadata": {},
   "source": [
    "##### Analyse de la matrice de confusion et du rapport de classification\n",
    "\n",
    "✅ **Principaux constats :**\n",
    "- **Non-départ (classe majoritaire) :**\n",
    "  - Très bonne précision (**0,96**) : peu de faux positifs.\n",
    "  - Rappel plus faible (**0,66**) : environ un tiers des non-départs sont mal classés.\n",
    "\n",
    "- **Départ (classe minoritaire) :**\n",
    "  - Rappel élevé (**0,87**) : la plupart des départs sont détectés.\n",
    "  - Précision faible (**0,33**) : beaucoup de faux positifs.\n",
    "  - F1-score modeste (**0,48**) : équilibre imparfait entre précision et rappel.\n",
    "\n",
    "✅ **Performance globale :**\n",
    "- **Accuracy : 0,69** (mais peu informative en contexte déséquilibré).\n",
    "- **Macro moyenne F1 : 0,63** (moyenne non pondérée des classes).\n",
    "- **F2-score final : 0,655**, confirmant la priorité donnée au rappel.\n",
    "\n",
    "⚠️ **Points d’attention :**\n",
    "- La performance sur la classe \"Départ\" dépend fortement du rappel, au détriment de la précision.\n",
    "- Le nombre élevé de faux positifs (84) nécessitera une gestion attentive si le modèle est utilisé en production.\n",
    "- Le choix du seuil reflète une stratégie assumée : **mieux vaut alerter que rater un départ**.\n",
    "\n",
    "**Synthèse :**\n",
    "- Le modèle capte bien les départs, avec un rappel satisfaisant.\n",
    "- La précision reste limitée, ce qui implique une communication claire sur le taux de fausses alertes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02945b-15bb-47fc-a1da-1d397a61b442",
   "metadata": {},
   "source": [
    "## Feature Importance Native XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5511c31-8c7c-4cd1-b72d-2e7c7e5f89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Extraire l'estimateur final depuis le pipeline optimisé\n",
    "model_xgb = grid_search.best_estimator_.named_steps[\"modele_utilise\"]\n",
    "\n",
    "# Extraire les importances\n",
    "importances = model_xgb.feature_importances_\n",
    "\n",
    "# Récupérer les noms de colonnes finales avec une méthode robuste\n",
    "preprocessor = grid_search.best_estimator_.named_steps[\"preprocessing\"]\n",
    "\n",
    "feature_names = []\n",
    "\n",
    "for name, transformer, columns in preprocessor.transformers_:\n",
    "    # Ignore le 'drop'\n",
    "    if transformer == \"drop\":\n",
    "        continue\n",
    "    # Prend les colonnes telles quelles si passthrough\n",
    "    if transformer == \"passthrough\":\n",
    "        feature_names.extend(columns)\n",
    "        continue\n",
    "    # Cas d'un pipeline imbriqué\n",
    "    if hasattr(transformer, \"named_steps\"):\n",
    "        last_step = list(transformer.named_steps.values())[-1]\n",
    "        if hasattr(last_step, \"get_feature_names_out\"):\n",
    "            names = last_step.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "    else:\n",
    "        if hasattr(transformer, \"get_feature_names_out\"):\n",
    "            names = transformer.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "    feature_names.extend(names)\n",
    "\n",
    "# Créer un DataFrame trié\n",
    "import pandas as pd\n",
    "\n",
    "df_importances = pd.DataFrame({\n",
    "    \"Variable\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Afficher le tableau\n",
    "print(df_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746c754-868a-443e-ae8d-3150831a6af6",
   "metadata": {},
   "source": [
    "##### Analyse importance des variables (importance native)\n",
    "\n",
    "- Les variables les plus importantes sont :\n",
    "  - **interaction_distance_heures_sup** et **interaction_satisfaction_anciennete** (~0,034).\n",
    "  - **age_revenu** et plusieurs variables liées au **poste occupé** et au **département** (~0,03).\n",
    "- D’autres variables comportementales et de satisfaction apparaissent dans le haut du classement (**satisfaction_moyenne**, **surmenage_transports**).\n",
    "- Plusieurs variables ont une importance nulle, notamment :\n",
    "  - **heures_supplementaires_binaire**, certaines **tranches d’âge**, et le **domaine d’étude RH**.\n",
    "- Ces résultats reflètent surtout l’impact structurel du modèle sur les splits des arbres.\n",
    "\n",
    "**Synthèse :**\n",
    "Cette importance native oriente la compréhension des principaux facteurs, mais sera complétée par l’analyse par permutation plus robuste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0f35c-90ac-40a7-bec6-6c3125e49d79",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31871590-ede8-4716-9205-3ba98dc6971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer l'importance par permutation\n",
    "result = permutation_importance(\n",
    "    grid_search.best_estimator_,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    scoring=f2_scorer,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Utiliser directement les colonnes originales de X_test\n",
    "feature_names_perm = X_test.columns\n",
    "\n",
    "# Vérification\n",
    "print(\"Nombre de colonnes X_test:\", len(feature_names_perm))\n",
    "print(\"Nombre d'importances:\", len(result.importances_mean))\n",
    "\n",
    "# Créer le DataFrame\n",
    "importances_perm_df = pd.DataFrame({\n",
    "    \"Variable\": feature_names_perm,\n",
    "    \"Importance moyenne\": result.importances_mean,\n",
    "    \"Écart-type\": result.importances_std\n",
    "}).sort_values(by=\"Importance moyenne\", ascending=False)\n",
    "\n",
    "# Afficher\n",
    "print(importances_perm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af8208-5ccb-4188-87e4-5bb7d24bf618",
   "metadata": {},
   "source": [
    "##### Synthèse globale des importances (native et permutation)\n",
    "\n",
    "✅ **Variables les plus contributives :**\n",
    "- Les deux méthodes confirment l’importance de :\n",
    "  - **interaction_distance_heures_sup** (native : top 1, permutation : plus forte importance).\n",
    "  - **surmenage_transports** (importante en permutation, bien classée en native).\n",
    "- Ces variables traduisent le lien entre contraintes de déplacement, surcharge et départ.\n",
    "\n",
    "✅ **Différences notables entre méthodes :**\n",
    "- La permutation valorise beaucoup plus **nombre_participation_pee** et **tranche_age**.\n",
    "- Des variables importantes en importance native (ex. `age_revenu`, certaines modalités de poste) apparaissent avec des importances négatives ou nulles en permutation.\n",
    "  - Cela indique qu’elles structurent les splits mais apportent moins d’amélioration de la performance globale.\n",
    "\n",
    "✅ **Variables avec importance nulle ou négative :**\n",
    "- Plusieurs variables affichent des valeurs nulles ou négatives en permutation :\n",
    "  - **revenu_stable**, **heures_supplementaires_binaire**, **satisfaction_employee**, etc.\n",
    "- Cela peut signifier qu’elles n’apportent pas d’information prédictive après prise en compte des autres variables.\n",
    "\n",
    "**Synthèse :**\n",
    "- L’importance par permutation est généralement plus fiable pour juger la **vraie contribution prédictive**.\n",
    "- La convergence sur certaines variables-clés renforce leur crédibilité (interactions et surcharge).\n",
    "- Les autres variables devront être validées ou écartées selon leur intérêt métier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdd39d-f2f0-4ddc-9197-dddebd96f5db",
   "metadata": {},
   "source": [
    "## Interprétation SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ed920-c123-4be4-93e3-983ae454973b",
   "metadata": {},
   "source": [
    "### Importance globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca022f1e-c63a-4517-bca4-29c34ad44daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire le modèle XGBoost depuis le pipeline\n",
    "model_xgb = grid_search.best_estimator_.named_steps[\"modele_utilise\"]\n",
    "\n",
    "# Transformer X_test avec le preprocessor\n",
    "X_test_transformed = grid_search.best_estimator_.named_steps[\"preprocessing\"].transform(X_test)\n",
    "\n",
    "# Créer l'explainer spécifique à XGBoost\n",
    "explainer = shap.TreeExplainer(model_xgb)\n",
    "\n",
    "# Calculer les valeurs SHAP\n",
    "shap_values = explainer.shap_values(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141860e1-e7c7-4d65-9a64-054a0690b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = grid_search.best_estimator_.named_steps[\"preprocessing\"]\n",
    "feature_names = []\n",
    "\n",
    "for name, transformer, columns in preprocessor.transformers_:\n",
    "    if transformer == \"drop\":\n",
    "        continue\n",
    "    if transformer == \"passthrough\":\n",
    "        feature_names.extend(columns)\n",
    "        continue\n",
    "    if hasattr(transformer, \"named_steps\"):\n",
    "        last_step = list(transformer.named_steps.values())[-1]\n",
    "        if hasattr(last_step, \"get_feature_names_out\"):\n",
    "            names = last_step.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "    else:\n",
    "        if hasattr(transformer, \"get_feature_names_out\"):\n",
    "            names = transformer.get_feature_names_out()\n",
    "        else:\n",
    "            names = columns\n",
    "    feature_names.extend(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb4c02a-fca1-4c7a-874f-f689be1a2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier la forme des shap_values\n",
    "print(type(shap_values))\n",
    "if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "    # Pour la classe positive\n",
    "    shap_values_class1 = shap_values[1]\n",
    "else:\n",
    "    # Si shap_values est un seul array\n",
    "    shap_values_class1 = shap_values\n",
    "\n",
    "df_shap = pd.DataFrame(\n",
    "    shap_values_class1,\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "# Aperçu des valeurs SHAP\n",
    "print(df_shap.head())\n",
    "\n",
    "df_shap.to_csv(\"shap_values_class1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95648f76-38d0-43e9-a2eb-93ac28bc3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap.abs().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ee0be-bc71-4826-a983-9bee55ca0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm plot\n",
    "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c95001-2cf9-407d-8c59-9683883078c6",
   "metadata": {},
   "source": [
    "##### Analyse des importances SHAP (valeurs absolues)\n",
    "\n",
    "✅ **Variables les plus contributives :**\n",
    "- **interaction_distance_heures_sup** (0,456) est la variable avec l’impact moyen le plus élevé.\n",
    "- **nombre_participation_pee** (0,430) et **age_revenu** (0,370) confirment leur poids prédictif important.\n",
    "- **satisfaction_moyenne** (0,343) et **interaction_satisfaction_anciennete** (0,314) occupent aussi une place clé.\n",
    "  - Ces variables traduisent des dimensions combinées de surcharge, stabilité et satisfaction.\n",
    "\n",
    "✅ **Variables secondaires mais significatives :**\n",
    "- **taux_de_formation**, **revenu_mensuel**, et **frequence_deplacement** apportent une contribution notable (0,22–0,24).\n",
    "- Certaines modalités de poste (**Assistant de Direction**, **Consultant**, **Cadre Commercial**) apparaissent également avec des valeurs SHAP moyennes >0,07.\n",
    "\n",
    "✅ **Variables à impact faible ou nul :**\n",
    "- De nombreuses variables affichent des valeurs SHAP très basses (<0,02), voire nulles :\n",
    "  - **heures_supplementaires_binaire**, **tranches d’âge Junior/Senior**, **Ressources Humaines**.\n",
    "  - Leur apport explicatif est négligeable.\n",
    "\n",
    "**Synthèse :**\n",
    "- L’analyse SHAP complète les autres méthodes (importance native, permutation) en confirmant le rôle dominant :\n",
    "  - Des variables d’**interaction et surcharge**.\n",
    "  - De la **satisfaction** et de l’**ancienneté combinée au revenu**.\n",
    "- La convergence entre plusieurs approches renforce la confiance dans la sélection de ces variables comme facteurs clés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c8e4a-f816-47c8-bd01-074feed82995",
   "metadata": {},
   "source": [
    "### Quelques exemples issues des deux classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30436c9d-0a38-49fe-a3b9-cecb64118e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les prédictions finales avec le seuil optimal\n",
    "y_pred_final = (y_scores_test >= best_threshold).astype(int)\n",
    "\n",
    "# Créer un DataFrame récapitulatif\n",
    "df_preds = pd.DataFrame({\n",
    "    \"Réel\": y_test.values,\n",
    "    \"Prédit\": y_pred_final,\n",
    "    \"Probabilité\": y_scores_test\n",
    "})\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(df_preds.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef734f-66d5-4bb7-8e42-1dd58d12dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation 4 : vrai positif\n",
    "shap.plots._waterfall.waterfall_legacy(\n",
    "    explainer.expected_value,\n",
    "    shap_values[4,:],\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faeb27b-d6f2-4dc5-9144-75cc8a9f27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation 0 : faux positif\n",
    "shap.plots._waterfall.waterfall_legacy(\n",
    "    explainer.expected_value,\n",
    "    shap_values[0,:],\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63479ec5-3e5b-4210-91d5-263b51aedca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation 1 : vrai négatif\n",
    "shap.plots._waterfall.waterfall_legacy(\n",
    "    explainer.expected_value,\n",
    "    shap_values[1,:],\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdaa80f-b9e1-4fd5-a25a-dd9ed949d17b",
   "metadata": {},
   "source": [
    "# Synthèse globale de la classification du turnover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcf5aa-00d4-4895-9ffd-dd455760cd4b",
   "metadata": {},
   "source": [
    "##### 🎯 Synthèse finale du projet\n",
    "\n",
    "---\n",
    "\n",
    "##### ✅ Performances globales du modèle\n",
    "\n",
    "- **Modèle retenu :**\n",
    "  - XGBoostClassifier\n",
    "  - Hyperparamètres finaux :\n",
    "    - `max_depth=3`\n",
    "    - `learning_rate=0.1`\n",
    "    - `subsample=0.5`\n",
    "    - `scale_pos_weight=5`\n",
    "- **Seuil optimal choisi :** ≈0.24\n",
    "- **Scores en test :**\n",
    "  - Recall classe Départ : **0.87**\n",
    "  - Precision classe Départ : **0.33**\n",
    "  - F2-score final : **0.655**\n",
    "- **Interprétation :**\n",
    "  - Le modèle privilégie la détection des départs (rappel élevé).\n",
    "  - Il accepte un nombre important de faux positifs.\n",
    "  - Ce compromis est assumé compte tenu de l’objectif métier.\n",
    "\n",
    "---\n",
    "\n",
    "##### ✅ Facteurs contributifs identifiés\n",
    "\n",
    "**1️⃣ Importance native**\n",
    "- Variables dominantes dans les arbres :\n",
    "  - `interaction_distance_heures_sup`\n",
    "  - `surmenage_transports`\n",
    "  - `age_revenu`\n",
    "- Ces variables structurent la construction des splits.\n",
    "\n",
    "**2️⃣ Importance par permutation**\n",
    "- Variables les plus contributives :\n",
    "  - `interaction_distance_heures_sup`\n",
    "  - `surmenage_transports`\n",
    "  - `nombre_participation_pee`\n",
    "- Certaines variables jugées importantes en native apparaissent faibles en permutation.\n",
    "\n",
    "**3️⃣ SHAP valeurs globales**\n",
    "- Variables à impact moyen élevé :\n",
    "  - `interaction_distance_heures_sup`\n",
    "  - `nombre_participation_pee`\n",
    "  - `satisfaction_moyenne`\n",
    "  - `age_revenu`\n",
    "- Les effets varient selon les individus.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🧭 Interprétation critique\n",
    "\n",
    "- Les trois méthodes convergent sur un constat :\n",
    "  **le modèle capte un signal dispersé, sans variable unique déterminante.**\n",
    "- Ce constat est fréquent en RH :\n",
    "  - Les causes de départ sont multifactorielles.\n",
    "  - Certaines dimensions (climat, motivations personnelles) ne figurent pas dans le dataset.\n",
    "- Les corrélations sont faibles à modérées.\n",
    "\n",
    "---\n",
    "\n",
    "##### 🧭 Recommandations métier prudentes\n",
    "\n",
    "1️⃣ **Surveiller les signaux faibles**\n",
    "   - Contraintes et surcharge (`interaction_distance_heures_sup`, `surmenage_transports`)\n",
    "   - Indicateurs de stabilité (`revenu_mensuel`, `age_revenu`)\n",
    "   - Participation aux dispositifs collectifs (`nombre_participation_pee`)\n",
    "\n",
    "2️⃣ **Compléter l’analyse par des données terrain**\n",
    "   - Entretiens de départ\n",
    "   - Enquêtes d’engagement\n",
    "\n",
    "3️⃣ **Renforcer le feature engineering**\n",
    "   - Intégrer des évolutions temporelles et des interactions supplémentaires.\n",
    "\n",
    "4️⃣ **Ne pas surinterpréter**\n",
    "   - Les prédictions sont une aide à la réflexion, non un jugement définitif.\n",
    "\n",
    "---\n",
    "\n",
    "##### 📝 Conclusion\n",
    "\n",
    "Le projet montre :\n",
    "- La faisabilité d’un modèle prédictif reproductible.\n",
    "- La capacité à identifier partiellement les profils à risque.\n",
    "- La nécessité de combiner approche quantitative et vision qualitative.\n",
    "\n",
    "**En synthèse :**\n",
    "> Ce modèle est un point de départ utile, mais doit être complété par une réflexion plus large sur les leviers d’engagement et de fidélisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e9f20-728e-454a-b21b-b93c82c3a338",
   "metadata": {},
   "source": [
    "## Export du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4e2dc-2be1-45cb-9a20-37df116107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export du pipeline\n",
    "joblib.dump(\n",
    "    grid_search.best_estimator_,\n",
    "    \"C:/Users/Fab/Documents/P5_Déployez_un_modèle_de_Machine_Learning/app/models/pipeline.joblib\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fa0f7-ab70-42d3-981d-3787ec191dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
